{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034367fd",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Analysis Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515ca00a",
   "metadata": {},
   "source": [
    "[Data Source](https://data.world/crowdflower/brands-and-product-emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044db6e",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "Our goal is to build a proof-of-concept classifier which would allow Apple to monitor sentiment toward its products (and the products of its top competitors) on Twitter.\n",
    "\n",
    "In particular, Apple needs to be able to monitor *negative sentiment* towards its products and that of its competitors to respond quickly to customer dissatisfaction to resolve the issue (or, in the case of its competitors, to proactively avoid their pain points.)\n",
    "\n",
    "## Data Understanding\n",
    "\n",
    "We have a collection of ~9,000 tweets concerning Apple, Android and Google products with the sentiment labeled (\"positive\", \"negative\", \"neutral\", or \"I can't tell\") and the object of the sentiment in the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42710d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "6e4f2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed for reproducibility\n",
    "seed = 3490\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972be2a9",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63d4ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text   \n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  \\\n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at   \n",
       "0                          iPhone  \\\n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from file; encoding is \"latin1\"\n",
    "data = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding='latin1')\n",
    "\n",
    "# inspect first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d5311ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8936, 3)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd9eaa",
   "metadata": {},
   "source": [
    "How many tweets contains #SXSW or #sxsw? (Answer: most of them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "41728d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = data.copy()\n",
    "data_list['tweet_words'] = data_list.tweet_text.map(lambda x: x.split())\n",
    "exploded = data_list.reset_index().rename({'index': 'true_index'}, axis=1).explode('tweet_words')\n",
    "about_sxsw = exploded.loc[(exploded.tweet_words == '#SXSW') | (exploded.tweet_words == '#sxsw')].true_index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "dc43a1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for #sxsw talk about #google doodles with @mention It will be fun! Friday, 5pm: {link}\n",
      "Why is wifi working on my laptop but neither that nor 3g on my iphone? grrr. #sxsw\n",
      "RT @mention Come check out @mention iPad App demos at #ARM's #SXSWparty on Mon! {link} #SXSW #SXSW2011\n",
      "@mention iPhone app is leading the pack, but android (blue) users #s are growing #sxswmobileapps #sxsw {link}\n",
      "Quoi de neuf sur #Blogger ? | WhatÛªs new with Blogger | Official #Google Blog {link} #SXSW\n"
     ]
    }
   ],
   "source": [
    "for tweet in data.tweet_text.loc[about_sxsw].sample(5):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24582b16",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0de4e0",
   "metadata": {},
   "source": [
    "The column names are a mouthful (\"is_there_an_emotion_directed_at_a_brand_or_product\"), so let's rename them for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8180b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>object_of_sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text object_of_sentiment   \n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone  \\\n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "          sentiment  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename last two columns, in place\n",
    "data.rename({'emotion_in_tweet_is_directed_at': 'object_of_sentiment',\n",
    "             'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'}, axis=1, inplace=True)\n",
    "# inspect first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65df63e",
   "metadata": {},
   "source": [
    "Now let's explore the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0efce112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec636d",
   "metadata": {},
   "source": [
    "Sample a few tweets which have unlabeled sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8aaa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finally got my #SXSW schedule on my #google calendar. #coudbeeasier\n",
      "funny! iphone correction? RT @mention Dashing off to learn a thing or two about longhorn journalism #SXSW\n",
      "&quot;Do you know what Apple is really good at? Making you feel bad about your Xmas present!&quot; - Seth Meyers on iPad2 #sxsw #doyoureallyneedthat?\n",
      "+1 ÛÏ@mention +1 RT @mention Petricone says Google TV is just a browser. I don't think that's correct. #SXSWÛ\n",
      "RT @mention Gary Vaynerchuck lÌ_gger ner winelibrary.tv och slÌ_pper i stÌ_llet Iphone-appen Daily Grape. #thankyoueconomy #sxsw #swesxsw\n"
     ]
    }
   ],
   "source": [
    "for i in data.loc[data.sentiment==\"I can't tell\"].sample(5, random_state=seed).index:\n",
    "    print(data.loc[i, 'tweet_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015296e2",
   "metadata": {},
   "source": [
    "Only one of these is clearly negative. However, forcing the classifier to treat them as positive, negative or neutral would probably yield some valuable insights for the brand (example: why don't you use your iPad on the go? Can the iPhone alarm clock be improved?) \\*Sorry, I didn't set the random_state initially! These comments were in reference to a different sample of tweets.\n",
    "\n",
    "With that being said, the records with unlabeled sentiment represent only a small portion of the dataset, and introducing semi-supervised labels here may have an unexpected effect given that the original labellers already classified these tweets as ambiguous. I will drop these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d097f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"I can't tell\" in sentiment column with null\n",
    "data.sentiment = data.sentiment.replace({\"I can't tell\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9b3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop records with null value for sentiment\n",
    "data.dropna(subset='sentiment', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5142be",
   "metadata": {},
   "source": [
    "Now let's explore the distribution of products discussed in these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82473539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Objects of Sentiment in Tweets')\n",
    "\n",
    "data.object_of_sentiment.value_counts().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c0a74",
   "metadata": {},
   "source": [
    "We have the most data on Apple products, and a little sprinkling of data on Google and Android products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa5d4a",
   "metadata": {},
   "source": [
    "What is the sentiment breakdown by object?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6285d8",
   "metadata": {},
   "source": [
    "Before we answer this question, first note that more than half of records do not have an object of sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8492192b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5655"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.object_of_sentiment.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d128ceb",
   "metadata": {},
   "source": [
    "Check how many of these null sentiment records correspond to tweets which are neutral or unlabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6582c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5298"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[(data.sentiment=='No emotion toward brand or product') | (data.sentiment==\"I can't tell\"),\n",
    "         'object_of_sentiment'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5cca4",
   "metadata": {},
   "source": [
    "There are 5298 records for which sentiment is unknown or neutral, and the \"object_of_sentiment\" value is null. But this means that there are a few records for which no object is given, despite a positive or negative sentiment being recorded.\n",
    "\n",
    "Let's view an example of such a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8274131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My iPhone says it can't connect to the Internet even though #sxsw wifi works great on my computer. Any suggestions?\n",
      "RT @mention The Web DesignerÛªs Guide to iOS (and Android) Apps, today @mention 10 a.m! {link} #sxsw\n",
      "Imagine if every company had the 20% concept like Google. #bavcID #SXSW\n"
     ]
    }
   ],
   "source": [
    "for i in data.loc[((data.sentiment=='Positive emotion') | (data.sentiment=='Negative emotion')) &\n",
    "                  (data.object_of_sentiment.isna())].sample(3, random_state=seed).index:\n",
    "    print(data.loc[i, 'tweet_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ed178",
   "metadata": {},
   "source": [
    "One approach to deal with these null objects is to classify them using the labeled data as training data (semi-supervised learning.)\n",
    "\n",
    "We may mis-classify some tweets -- for example, tweet 64 is not directed at a specific Apple product at all -- but hopefully we can capture the fact that some of these *are* clearly directed at specific products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b42a3a",
   "metadata": {},
   "source": [
    "Before we complete the above semi-supervised learning task for the \"object_of_sentiment\" category, let's look at the current distribution of positive, negative and neutral tweets by product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9fac747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            sentiment  count\n",
       "0                    Positive emotion    793\n",
       "1                    Negative emotion    125\n",
       "2  No emotion toward brand or product     24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.object_of_sentiment=='iPad', 'sentiment'].value_counts().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c04ac08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\n",
    "\n",
    "# set up color map for negative, positive and neutral tweets\n",
    "viridis = mpl.colormaps['viridis'].resampled(3)\n",
    "\n",
    "for n, product in enumerate(np.unique(data.object_of_sentiment.to_list())[:-1]):\n",
    "    # get sentiment counts for this product\n",
    "    df = data.loc[data.object_of_sentiment==product, 'sentiment'].value_counts().to_frame().reset_index()\n",
    "\n",
    "    # plot bar chart\n",
    "    df.plot(kind='barh', ax=ax[n//3][n%3])\n",
    "    \n",
    "    # set colors for each bar according to whether it is the count of positive, negative or neutral tweets\n",
    "    bars = [i for i in ax[n//3][n%3].containers if isinstance(i, BarContainer)]\n",
    "    colors = df.sentiment.replace({'Positive emotion': 2,\n",
    "                                   'Negative emotion': 0,\n",
    "                                   'No emotion toward brand or product': 1})\n",
    "    for i in range(3):\n",
    "        bars[0][i].set_color(viridis(colors[i]/2))\n",
    "    \n",
    "    # fix labels, remove legend\n",
    "    ax[n//3][n%3].set_yticklabels(df['sentiment'])\n",
    "    ax[n//3][n%3].get_legend().remove()\n",
    "    \n",
    "    # set title\n",
    "    ax[n//3][n%3].set_title(f'{product}')\n",
    "    \n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09663ce",
   "metadata": {},
   "source": [
    "The iPhone has the highest ratio of negative to positive emotion tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da3cc6",
   "metadata": {},
   "source": [
    "Let's dig in a little more to customer sentiment toward Apple products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "7eebc536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>object_of_sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text object_of_sentiment   \n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone  \\\n",
       "\n",
       "          sentiment  \n",
       "0  Negative emotion  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "44720a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative emotion toward iPhone:\n",
      "The iPhone battery was not made for @mention #sxsw\n",
      "\n",
      "\n",
      "No emotion toward brand or product toward iPhone:\n",
      "@mention I'm experimenting doing #SXSW with just a paper notebook, iPad and iPhone. No &quot;traditional&quot; laptop.\n",
      "\n",
      "\n",
      "Positive emotion toward iPhone:\n",
      "Love that I have a MacBook, iPad, and iPhone with me at #sxsw this year. One runs out of juice, and I can jump to the next.\n",
      "\n",
      "\n",
      "Negative emotion toward iPad or iPhone App:\n",
      "@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw\n",
      "\n",
      "\n",
      "No emotion toward brand or product toward iPad or iPhone App:\n",
      "New #UberSocial for #iPhone now in the App Store includes UberGuide to #SXSW sponsored by #Mashable {link} by @mention #app\n",
      "\n",
      "\n",
      "Positive emotion toward iPad or iPhone App:\n",
      "RT @mention Updated NPR Music iPhone app has song info for All Songs 24/7 &amp; live video streaming just in time for #SXSW {link}\n",
      "\n",
      "\n",
      "Negative emotion toward iPad:\n",
      "hey @mention twitter needs a way for us in disaster areas to filter out things like #sxsw and #ipad too because right now we really don't care\n",
      "\n",
      "\n",
      "No emotion toward brand or product toward iPad:\n",
      "RT @mention I'm not really at #sxsw. Just messing with you. I'm making money instead. // I bet someone left the iPad queue\n",
      "\n",
      "\n",
      "Positive emotion toward iPad:\n",
      "I went to #sxswi and all I won was this lousy #iPad #sxsw :-) :-) {link}\n",
      "\n",
      "\n",
      "Negative emotion toward Other Apple product or service:\n",
      "Design for iPad is like a design 101 class. Will someone give a talk and assume that we didn't all ditch our previous experience #sxsw\n",
      "\n",
      "\n",
      "No emotion toward brand or product toward Other Apple product or service:\n",
      "From #Apple to Naomi Campbell: pop-up stores are all the rage: {link} #sxsw\n",
      "\n",
      "\n",
      "Positive emotion toward Other Apple product or service:\n",
      "RT @mention Check out the FREE @mention Sampler on iTunes! {link} #SXSW\n",
      "\n",
      "\n",
      "Negative emotion toward Apple:\n",
      "RT @mention I have yet to walk into a conference room where it doesn't look like an Apple ad. You'd think there was nothing else. #SXSW\n",
      "\n",
      "\n",
      "No emotion toward brand or product toward Apple:\n",
      "The #SXSW Apple &quot;pop-up&quot; store was nuts. Was still packed at 11:30. {link}\n",
      "\n",
      "\n",
      "Positive emotion toward Apple:\n",
      "@mention it is a good question &amp; for most brands, the answer is probably no! Except apple of course..maybe Netflix too?. #SxSW\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apple_products = ['iPhone', 'iPad or iPhone App', 'iPad', 'Other Apple product or service', 'Apple']\n",
    "sentiments = np.unique(data.sentiment)\n",
    "\n",
    "for product in apple_products:\n",
    "    for sentiment in sentiments:\n",
    "        try:\n",
    "            print(f'{sentiment} toward {product}:')\n",
    "            text = data.loc[\n",
    "                (data.object_of_sentiment==product)&(data.sentiment==sentiment), 'tweet_text'].sample(1).values[0]\n",
    "            print(text)\n",
    "            print('\\n')\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308b11a",
   "metadata": {},
   "source": [
    "How many tweets are concerning Apple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "712a40d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2402, 3)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.object_of_sentiment.isin(apple_products)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "97b89376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2688003581020591"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2402 / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93fafc",
   "metadata": {},
   "source": [
    "Roughly 27% of our data concerns Apple products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69477444",
   "metadata": {},
   "source": [
    "## Semi-supervised Learning\n",
    "\n",
    "Apply semi-supervised learning to classify the object_of_sentiment for tweets with positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fe3d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>object_of_sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text object_of_sentiment   \n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone  \\\n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "          sentiment  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all tweets with positive or negative sentiment\n",
    "emotive_tweets = data.loc[(data.sentiment=='Positive emotion') | (data.sentiment=='Negative emotion')]\n",
    "emotive_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06080b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into \"train\" and \"predict\"\n",
    "train = emotive_tweets.loc[emotive_tweets.object_of_sentiment.isna()==False]\n",
    "predict = emotive_tweets.loc[emotive_tweets.object_of_sentiment.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564e3ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shapes\n",
    "train.shape[0] + predict.shape[0] == emotive_tweets.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb8251",
   "metadata": {},
   "source": [
    "Now, we want to build a classifier to classify the object_of_sentiment for the \"predict\" set based on the labels in the \"train\" set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8a47452",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train['object_of_sentiment']\n",
    "train_features = train['tweet_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f96ad",
   "metadata": {},
   "source": [
    "We need to label encode the target, then vectorize the tweet text.\n",
    "\n",
    "We now run into the problem that we are going to have when building the sentiment classifier; there are many different ways that we could preprocess the tweet text prior to modeling, and when you combine this with all of the classification algorithms we could use and their hyperparameters to be tuned, we have a truly large search space for the \"best\" model to do the job.\n",
    "\n",
    "For this reason, I plan to drop this data for now and return to this piece of the project after building out a classifier below. I will use whatever preprocessing techniques resulted in the best performance for identifying the object of the sentiment, and I will apply that preprocessing technique to the \"training data\" for labeling the unlabeled object_of_sentiment values, then build a classifier to label these missing values.\n",
    "\n",
    "When I add the newly labeled data to the rest of the training data and refit the best-performing classifier as determined below, if the performance improves or stays the same, I will keep it. Otherwise, this data will be permanently dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e3b5b",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39b313",
   "metadata": {},
   "source": [
    "Our goal is to build a classifier which identifies the i.) sentiment and ii.) object of sentiment of a tweet.\n",
    "\n",
    "We are going to build the classifier in steps:\n",
    "\n",
    "1.) Classify a tweet as negative or not negative (positive or neutral, with no distinction between the two.)\n",
    "\n",
    "2.) Classify the object of the sentiment in a tweet.\n",
    "\n",
    "3.) Combine 1 and 2.\n",
    "\n",
    "4.) Classify a tweet as positive, negative or neutral and identify the object of the sentiment (if applicable.)\n",
    "\n",
    "It is not critically important that our classifier be able to distinguish between positive and neutral tweets (hence, it is the last step in our process.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "fceaa689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                0\n",
       "object_of_sentiment    5654\n",
       "sentiment                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "1d4f878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 8936 records.\n"
     ]
    }
   ],
   "source": [
    "# drop tweet that is missing tweet_text\n",
    "data.dropna(subset='tweet_text', inplace=True)\n",
    "print(f'We now have {data.shape[0]} records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6de5d",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b1975bfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5718,)\n",
      "(5718,)\n",
      "\n",
      "\n",
      "(1430,)\n",
      "(1430,)\n",
      "\n",
      "\n",
      "(1788,)\n",
      "(1788,)\n"
     ]
    }
   ],
   "source": [
    "# separate features and target\n",
    "X = data['tweet_text']\n",
    "y = data['sentiment']\n",
    "\n",
    "# label encode the target\n",
    "le = LabelEncoder()\n",
    "y_encoded = pd.Series(le.fit_transform(y), index=y.index)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=seed)\n",
    "\n",
    "# train val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n",
    "\n",
    "# check shapes\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('\\n')\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print('\\n')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "f3167720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.602134\n",
       "2    0.335607\n",
       "0    0.062260\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "c6555a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Negative emotion',\n",
       " 1: 'No emotion toward brand or product',\n",
       " 2: 'Positive emotion'}"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip([0, 1, 2], le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1df65",
   "metadata": {},
   "source": [
    "We may have some difficulty capturing the negative class due to its tiny minority! We may even have some difficulty capturing the positive class, also; the neutral class is so big."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e48dc",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e3597",
   "metadata": {},
   "source": [
    "I am going to try three different methods of preprocessing the tweet text to pass in to the model. The tweets need to be \"clean\" (lower cased, with punctuation removed and, ideally, some form of tokenization applied) before these methods can be applied, so we are going to do that preprocessing work first. We will also remove some common English stop words.\n",
    "\n",
    "The vectorization / embedding techniques to then be applied are:\n",
    "- count or term frequency inverse document frequency vectorization (using Scikit Learn's implementations; we will try both)\n",
    "- using pretrained GloVe embeddings and a mean word embedding for each tweet\n",
    "- training a Word2Vec model on our corpus to obtain embeddings (using gensim) and using a mean word embedding for each tweet\n",
    "\n",
    "A fourth and final possible technique to be applied is stacking a combination of these embeddings/vectorizations to pass into the model, along with any other potentially helpful features (e.g. TextBlob's sentiment analysis polarity results for the tweet.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f9cdc",
   "metadata": {},
   "source": [
    "#### General Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b1e551",
   "metadata": {},
   "source": [
    "We can apply general clean up techniques and tokenization to the entire training data without leakage, so we will do that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac2f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up (remove punctuation & change to lower case) tweets & remove english stopwords\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def clean_up(tweet):\n",
    "    return ' '.join([word.strip(string.punctuation).lower() for word in tweet.split() if\n",
    "                     word not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7d05b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8841    next sxsw we're heading designing ipad apps ba...\n",
       "1099    talked great developers android meetup looking...\n",
       "3738          ok i definitely need ipad 2 sxsw gadgetenvy\n",
       "4721    anybody know whether i nab white 3g 64gb ipad2...\n",
       "4200    app called your hands sxsw rt mention need app...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.apply(clean_up).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eead14",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56139d57",
   "metadata": {},
   "source": [
    "We can employ stemming or lemmatization to obtain tokens for vectorization.\n",
    "\n",
    "Stemming is very simple with nltk's Porter Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "653b5d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi i'm rob and i'm go swim tomorrow woohoo\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def get_stems(tweet):\n",
    "    '''Takes in a string of words and outputs a string of stemmed tokens.'''\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in tweet.split()])\n",
    "\n",
    "# test\n",
    "get_stems(\"hi i'm rob and i'm going swimming tomorrow woohoo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be843d9",
   "metadata": {},
   "source": [
    "Lemmatization looks more complicated, but this is only because the lemmatizer from WordNet works best if part-of-speech tags are passed to it along with the word itself. Most of the code below generates a wordnet version of a part of speech tag for a word and passes it in to the lemmatizer along with the word itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87e71864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi i'm rob and i'm go swim tomorrow woohoo\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization\n",
    "\n",
    "# imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# helper function to get wordnet pos_tags from nltk pos_tags\n",
    "def get_wordnet_pos(tag):\n",
    "    '''Convert POS tags generated by nltk.pos_tag to wordnet tags, for use with word net\n",
    "    lemmatizer provided in the nltk.stem.wordnet package.'''\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# define function to apply lemmatizer to wordnet-tagged words to get lemmas\n",
    "def get_lemmas(tweet):\n",
    "    '''Takes in a string (tweet) and returns string of lemmatized tokens.'''\n",
    "    # instantiate word net lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    nltk_pos_tags = nltk.pos_tag(tweet.split()) # list of (word, pos_tag)\n",
    "    \n",
    "    wordnet_pos_tags = [(pair[0], get_wordnet_pos(pair[1])) for pair in nltk_pos_tags]\n",
    "    \n",
    "    return ' '.join([lemmatizer.lemmatize(*tagged_word) for tagged_word in wordnet_pos_tags])\n",
    "\n",
    "# test\n",
    "get_lemmas(\"hi i'm rob and i'm going swimming tomorrow woohoo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "d72592fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stemmed and lemmatized tokens for training & validation data\n",
    "X_train_stems = X_train.apply(clean_up).apply(get_stems)\n",
    "X_train_lemmas = X_train.apply(clean_up).apply(get_lemmas)\n",
    "\n",
    "X_val_stems = X_val.apply(clean_up).apply(get_stems)\n",
    "X_val_lemmas = X_val.apply(clean_up).apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "586f4ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5390    rt mention android develop friend let' hang 12...\n",
       "5237    rt mention ipad design tip look like physic ob...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stems.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "03b34973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5390    rt mention android developer friends let's han...\n",
       "5237    rt mention ipad design tip look like physical ...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lemmas.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbb7a6",
   "metadata": {},
   "source": [
    "We see an example of stemming versus lemmatization in tweet 5237: stemming results in \"physic\", lemmatization results in \"physical\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27587cf3",
   "metadata": {},
   "source": [
    "The classifiers we will test out with are preprocessed data are random forest, naive bayes and support vector machine. We are not testing logistic regression because it is not particularly suited to a multi-class classification problem, and we are not using KNN because it is possible that two tweets which are both positive will contain very different content.\n",
    "\n",
    "I will store my results in the `model_eval` dictionary below which can easily be displayed as a DataFrame to view the model tuning process over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "dc40be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = {'tokenization': [], 'preprocessing': [], 'classifier': [], 'score': []}\n",
    "\n",
    "def score_model(model, X_train, y_train, X_val, y_val, token, preprocess, clf):\n",
    "    global model_eval\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "    except:\n",
    "        print('Error fitting model; check training data. Is it scaled? Try fitting separately to troubleshoot.')\n",
    "        return 0\n",
    "    \n",
    "    model_eval['tokenization'].append(token)\n",
    "    model_eval['preprocessing'].append(preprocess)\n",
    "    model_eval['classifier'].append(clf)\n",
    "    model_eval['score'].append(model.score(X_val, y_val))\n",
    "    \n",
    "    return pd.DataFrame(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "a4df8c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt</th>\n",
       "      <th>mention</th>\n",
       "      <th>android</th>\n",
       "      <th>develop</th>\n",
       "      <th>friend</th>\n",
       "      <th>let</th>\n",
       "      <th>hang</th>\n",
       "      <th>12</th>\n",
       "      <th>30p</th>\n",
       "      <th>saturday</th>\n",
       "      <th>...</th>\n",
       "      <th>webmail</th>\n",
       "      <th>suppose</th>\n",
       "      <th>forb</th>\n",
       "      <th>mahalo</th>\n",
       "      <th>97</th>\n",
       "      <th>woop</th>\n",
       "      <th>youkidshavefun</th>\n",
       "      <th>tdg</th>\n",
       "      <th>exclud</th>\n",
       "      <th>interface</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 6512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rt  mention  android  develop  friend  let  hang  12  30p  saturday   \n",
       "5390   0        0        0        0       0    0     0   0    0         0  \\\n",
       "5237   0        0        0        0       0    0     0   0    0         0   \n",
       "\n",
       "      ...  webmail  suppose  forb  mahalo  97  woop  youkidshavefun  tdg   \n",
       "5390  ...        0        0     0       0   0     0               0    0  \\\n",
       "5237  ...        0        0     0       0   0     0               0    0   \n",
       "\n",
       "      exclud  interface  \n",
       "5390       0          0  \n",
       "5237       0          0  \n",
       "\n",
       "[2 rows x 6512 columns]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "\n",
    "final_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                                                    columns=count.vocabulary_)\n",
    "final_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val.index, columns=count.vocabulary_)\n",
    "\n",
    "final_X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29468229",
   "metadata": {},
   "source": [
    "For random forest and naive bayes, it is not strictly necessary to scale the data. Let's go ahead and get scores for these two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "9ecce406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokenization    preprocessing     classifier     score\n",
       "0         stem  count_vectorize  random_forest  0.676923\n",
       "1         stem  count_vectorize    naive_bayes  0.684615"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "nb = MultinomialNB()\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', 'count_vectorize', 'random_forest')\n",
    "score_model(nb, final_X_train, y_train, final_X_val, y_val, 'stem', 'count_vectorize', 'naive_bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c31ee",
   "metadata": {},
   "source": [
    "For a support vector machine classifier, we do need to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "16a19ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokenization    preprocessing      classifier     score\n",
       "0         stem  count_vectorize   random_forest  0.676923\n",
       "1         stem  count_vectorize     naive_bayes  0.684615\n",
       "2         stem  count_vectorize  support_vector  0.653147"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "scaled_X_train = ss.fit_transform(final_X_train)\n",
    "scaled_X_val = ss.transform(final_X_val)\n",
    "\n",
    "svc = SVC(random_state=seed)\n",
    "\n",
    "score_model(svc, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', 'count_vectorize', 'support_vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d34146",
   "metadata": {},
   "source": [
    "With stemming and count vectorization, Naive Bayes is in the lead.\n",
    "\n",
    "Now we need to tune these classifiers, then tune the preprocessing technique (count vectorization) itself.\n",
    "\n",
    "Before we do so, let's check the model scores with the same preprocessing technique, but lemmatization instead of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "966efc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokenization    preprocessing      classifier     score\n",
       "0         stem  count_vectorize   random_forest  0.676923\n",
       "1         stem  count_vectorize     naive_bayes  0.684615\n",
       "2         stem  count_vectorize  support_vector  0.653147\n",
       "3    lemmatize  count_vectorize   random_forest  0.679021\n",
       "4    lemmatize  count_vectorize     naive_bayes  0.683916\n",
       "5    lemmatize  count_vectorize  support_vector  0.647552"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit count vectorizer and standard scaler with lemmatized data\n",
    "\n",
    "final_X_train = pd.DataFrame(count.fit_transform(X_train_lemmas).todense(), index=X_train_lemmas.index,\n",
    "                                                    columns=count.vocabulary_)\n",
    "final_X_val = pd.DataFrame(count.transform(X_val_lemmas).todense(), index=X_val_lemmas.index,\n",
    "                           columns=count.vocabulary_)\n",
    "\n",
    "scaled_X_train = ss.fit_transform(final_X_train)\n",
    "scaled_X_val = ss.transform(final_X_val)\n",
    "\n",
    "# score models with new training and validation data\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'lemmatize', 'count_vectorize', 'random_forest')\n",
    "score_model(nb, final_X_train, y_train, final_X_val, y_val, 'lemmatize', 'count_vectorize', 'naive_bayes')\n",
    "score_model(svc, scaled_X_train, y_train, scaled_X_val, y_val, 'lemmatize', 'count_vectorize', 'support_vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0badaf9f",
   "metadata": {},
   "source": [
    "The best performing classifier so far is Naive Bayes, and it performs 0.1% better with stemming, rather than lemmatization, so we'll use the stemmed and count-vectorized data to tune our Naive Bayes classifier next. Then we can turn our attention back to tuning the count vectorizer itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "257acbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt</th>\n",
       "      <th>mention</th>\n",
       "      <th>android</th>\n",
       "      <th>develop</th>\n",
       "      <th>friend</th>\n",
       "      <th>let</th>\n",
       "      <th>hang</th>\n",
       "      <th>12</th>\n",
       "      <th>30p</th>\n",
       "      <th>saturday</th>\n",
       "      <th>...</th>\n",
       "      <th>webmail</th>\n",
       "      <th>suppose</th>\n",
       "      <th>forb</th>\n",
       "      <th>mahalo</th>\n",
       "      <th>97</th>\n",
       "      <th>woop</th>\n",
       "      <th>youkidshavefun</th>\n",
       "      <th>tdg</th>\n",
       "      <th>exclud</th>\n",
       "      <th>interface</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 6512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rt  mention  android  develop  friend  let  hang  12  30p  saturday   \n",
       "5390   0        0        0        0       0    0     0   0    0         0  \\\n",
       "5237   0        0        0        0       0    0     0   0    0         0   \n",
       "\n",
       "      ...  webmail  suppose  forb  mahalo  97  woop  youkidshavefun  tdg   \n",
       "5390  ...        0        0     0       0   0     0               0    0  \\\n",
       "5237  ...        0        0     0       0   0     0               0    0   \n",
       "\n",
       "      exclud  interface  \n",
       "5390       0          0  \n",
       "5237       0          0  \n",
       "\n",
       "[2 rows x 6512 columns]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                            columns=count.vocabulary_)\n",
    "best_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "                          columns=count.vocabulary_)\n",
    "best_X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "a525c4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=MultinomialNB(), param_grid={&#x27;alpha&#x27;: [1, 0.5, 10]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=MultinomialNB(), param_grid={&#x27;alpha&#x27;: [1, 0.5, 10]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=MultinomialNB(), param_grid={'alpha': [1, 0.5, 10]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'alpha': [1, 0.5, 10]}\n",
    "\n",
    "grid_search = GridSearchCV(nb, param_grid, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(best_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "c285ebae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6631672570648949"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461102d1",
   "metadata": {},
   "source": [
    "There is very little that can be done to tune a Naive Bayes classifier. Changing the smoothing parameter didn't have a great effect.\n",
    "\n",
    "So we'll move on to tuning the count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "b3eb56a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6512"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a8c7c",
   "metadata": {},
   "source": [
    "Currently, the \"baseline\" count vectorizer has a vocabulary of 6512 lemmas.\n",
    "\n",
    "One technique we can try to improve the classifier's performance is limiting the vectorizer's vocabulary to only those words which show up often enough *and* little enough to qualify as being meaningful. (Example: 'the' shows up too often to provide information, but a word which only shows up once in the entire corpus also won't help us, for example, a specific person's username.)\n",
    "\n",
    "The below vectorizer has a \"maximum document frequency\" of 90% (only include a token if it appears in fewer than 90% of tweets) and a \"minimum document frequency\" of 5% (only include tokens which appear in more than 5% of all tweets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "42d473cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokenization                             preprocessing       classifier   \n",
       "0         stem                           count_vectorize    random_forest  \\\n",
       "1         stem                           count_vectorize      naive_bayes   \n",
       "2         stem                           count_vectorize   support_vector   \n",
       "3    lemmatize                           count_vectorize    random_forest   \n",
       "4    lemmatize                           count_vectorize      naive_bayes   \n",
       "5    lemmatize                           count_vectorize   support_vector   \n",
       "6         stem  CountVectorizer(max_df=0.9, min_df=0.05)  MultinomialNB()   \n",
       "\n",
       "      score  \n",
       "0  0.676923  \n",
       "1  0.684615  \n",
       "2  0.653147  \n",
       "3  0.679021  \n",
       "4  0.683916  \n",
       "5  0.647552  \n",
       "6  0.612587  "
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(min_df=0.05, max_df=0.9)\n",
    "\n",
    "final_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                                                    columns=count.vocabulary_)\n",
    "final_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "                           columns=count.vocabulary_)\n",
    "\n",
    "score_model(nb, final_X_train, y_train, final_X_val, y_val, 'stem', count, nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e257e5b",
   "metadata": {},
   "source": [
    "Our model's score is worse (by about 7%.) Let's try a more conservative maximum document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "fd132a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokenization                              preprocessing       classifier   \n",
       "0         stem                            count_vectorize    random_forest  \\\n",
       "1         stem                            count_vectorize      naive_bayes   \n",
       "2         stem                            count_vectorize   support_vector   \n",
       "3    lemmatize                            count_vectorize    random_forest   \n",
       "4    lemmatize                            count_vectorize      naive_bayes   \n",
       "5    lemmatize                            count_vectorize   support_vector   \n",
       "6         stem   CountVectorizer(max_df=0.9, min_df=0.05)  MultinomialNB()   \n",
       "7         stem  CountVectorizer(max_df=0.95, min_df=0.05)  MultinomialNB()   \n",
       "\n",
       "      score  \n",
       "0  0.676923  \n",
       "1  0.684615  \n",
       "2  0.653147  \n",
       "3  0.679021  \n",
       "4  0.683916  \n",
       "5  0.647552  \n",
       "6  0.612587  \n",
       "7  0.612587  "
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(min_df=0.05, max_df=0.95)\n",
    "\n",
    "final_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                                                    columns=count.vocabulary_)\n",
    "final_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "                           columns=count.vocabulary_)\n",
    "\n",
    "score_model(nb, final_X_train, y_train, final_X_val, y_val, 'stem', count, nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff91a7ad",
   "metadata": {},
   "source": [
    "Taking out information doesn't seem to be improving the model's performance, so let's try adding information by including bigrams in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "f8fe4ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokenization                              preprocessing       classifier   \n",
       "0         stem                            count_vectorize    random_forest  \\\n",
       "1         stem                            count_vectorize      naive_bayes   \n",
       "2         stem                            count_vectorize   support_vector   \n",
       "3    lemmatize                            count_vectorize    random_forest   \n",
       "4    lemmatize                            count_vectorize      naive_bayes   \n",
       "5    lemmatize                            count_vectorize   support_vector   \n",
       "6         stem   CountVectorizer(max_df=0.9, min_df=0.05)  MultinomialNB()   \n",
       "7         stem  CountVectorizer(max_df=0.95, min_df=0.05)  MultinomialNB()   \n",
       "8         stem        CountVectorizer(ngram_range=(1, 2))  MultinomialNB()   \n",
       "\n",
       "      score  \n",
       "0  0.676923  \n",
       "1  0.684615  \n",
       "2  0.653147  \n",
       "3  0.679021  \n",
       "4  0.683916  \n",
       "5  0.647552  \n",
       "6  0.612587  \n",
       "7  0.612587  \n",
       "8  0.690909  "
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "final_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                                                    columns=count.vocabulary_)\n",
    "final_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "                           columns=count.vocabulary_)\n",
    "\n",
    "score_model(nb, final_X_train, y_train, final_X_val, y_val, 'stem', count, nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7692e83",
   "metadata": {},
   "source": [
    "Adding bigrams very slightly improved the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bbed4",
   "metadata": {},
   "source": [
    "It's probably worth giving our other two classifiers a chance to be tuned as well before dismissing them out of hand (especially since there was so little to tune in the best model - there are a lot more knobs to turn for random forest, so perhaps it will improve significantly over the baseline.)\n",
    "\n",
    "Going back to simply X_train_stems, let's tune our random forest classifier next.\n",
    "\n",
    "Increasing the number of estimators should improve performance. Limiting the maximum depth of individual decision trees can help those trees not to overfit. It's possible that weighting the classes according to their ratios could improve performance as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "21d9f98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6411328610147508"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300],\n",
    "              'max_depth': [2, 10, 20, 50],\n",
    "              'random_state': [seed]}\n",
    "\n",
    "RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(final_X_train, y_train)\n",
    "\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e89d0",
   "metadata": {},
   "source": [
    "Tuning the random forest failed miserably. Let's try different versions of count vectorize paired with the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "9526b06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokenization                              preprocessing   \n",
       "0         stem                            count_vectorize  \\\n",
       "1         stem                            count_vectorize   \n",
       "2         stem                            count_vectorize   \n",
       "3    lemmatize                            count_vectorize   \n",
       "4    lemmatize                            count_vectorize   \n",
       "5    lemmatize                            count_vectorize   \n",
       "6         stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7         stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9         stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "\n",
       "                                          classifier     score  \n",
       "0                                      random_forest  0.676923  \n",
       "1                                        naive_bayes  0.684615  \n",
       "2                                     support_vector  0.653147  \n",
       "3                                      random_forest  0.679021  \n",
       "4                                        naive_bayes  0.683916  \n",
       "5                                     support_vector  0.647552  \n",
       "6                                    MultinomialNB()  0.612587  \n",
       "7                                    MultinomialNB()  0.612587  \n",
       "8                                    MultinomialNB()  0.690909  \n",
       "9  (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  "
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(min_df=0.05, max_df=0.95)\n",
    "\n",
    "final_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                             columns=count.vocabulary_)\n",
    "final_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "                           columns=count.vocabulary_)\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', count, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d986d",
   "metadata": {},
   "source": [
    "Once again, let's try adding bigrams instead of specifying document frequency limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "5c1bdbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  "
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "final_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                             columns=count.vocabulary_)\n",
    "final_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "                           columns=count.vocabulary_)\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', count, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864f528",
   "metadata": {},
   "source": [
    "Let's tune our support vector machine, then move on to other preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "ce684e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too long\n",
    "\n",
    "# count = CountVectorizer()\n",
    "\n",
    "# final_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "#                              columns=count.vocabulary_)\n",
    "# final_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "#                            columns=count.vocabulary_)\n",
    "\n",
    "# scaled_X_train = pd.DataFrame(ss.fit_transform(final_X_train), index=final_X_train.index,\n",
    "#                               columns=final_X_train.columns)\n",
    "# scaled_X_val = pd.DataFrame(ss.transform(final_X_val), index=final_X_val.index,\n",
    "#                             columns=final_X_val.columns)\n",
    "\n",
    "# param_grid = [\n",
    "#     {'C': [0.01, 0.1, 1],\n",
    "#      'kernel': ['poly', 'rbf'],\n",
    "#      'gamma': [0.001, 0.005, 0.01],\n",
    "#      'class_weight': ['balanced', {0:1, 1:1, 2:1}]}\n",
    "# ]\n",
    "\n",
    "# grid_search = GridSearchCV(svc, param_grid, scoring='accuracy')\n",
    "\n",
    "# grid_search.fit(scaled_X_train, y_train)\n",
    "\n",
    "# grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "a54ffb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  "
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count = CountVectorizer()\n",
    "\n",
    "final_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                             columns=count.vocabulary_)\n",
    "final_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "                           columns=count.vocabulary_)\n",
    "\n",
    "scaled_X_train = pd.DataFrame(ss.fit_transform(final_X_train), index=final_X_train.index,\n",
    "                              columns=final_X_train.columns)\n",
    "scaled_X_val = pd.DataFrame(ss.transform(final_X_val), index=final_X_val.index,\n",
    "                            columns=final_X_val.columns)\n",
    "\n",
    "svc = SVC(kernel='poly')\n",
    "\n",
    "score_model(svc, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', count, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e426a7",
   "metadata": {},
   "source": [
    "Examine the performance of the best model so far (model 8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "55b114d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "count = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "final_X_train = pd.DataFrame(count.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                             columns=count.vocabulary_)\n",
    "final_X_val = pd.DataFrame(count.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "                           columns=count.vocabulary_)\n",
    "\n",
    "nb.fit(final_X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 8))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_val, nb.predict(final_X_val))).plot(ax=ax[0])\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_val, nb.predict(final_X_val), normalize='true')).plot(ax=ax[1])\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfa430",
   "metadata": {},
   "source": [
    "The best model's performance corresponds loosely to class size. Let's try undersampling to get a better rate of prediction for the positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "fdfbad1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3443\n",
       "2    1919\n",
       "0     356\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "09c7d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly undersample to make class weights equal\n",
    "\n",
    "neutral_index = (y_train.loc[(y_train == 1)]).sample(356, random_state=seed).index\n",
    "positive_index = (y_train.loc[(y_train == 2)]).sample(356, random_state=seed).index\n",
    "undersampled_index = np.concatenate((neutral_index, positive_index, (y_train.loc[(y_train == 0)]).index))\n",
    "\n",
    "y_train_undersampled = y_train.loc[undersampled_index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "70f19fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    356\n",
       "2    356\n",
       "0    356\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_undersampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "2c7828a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_undersampled = X_train_stems.loc[undersampled_index].copy()\n",
    "\n",
    "final_X_train_undersampled = pd.DataFrame(count.fit_transform(X_train_undersampled).todense(),\n",
    "                                          index=X_train_undersampled.index, columns=count.vocabulary_)\n",
    "final_X_val_us = pd.DataFrame(count.transform(X_val).todense(), index=X_val.index, columns=count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "eeb66db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6265734265734266"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(final_X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "nb.score(final_X_val_us, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5474ec78",
   "metadata": {},
   "source": [
    "Performance is worse. What about a different class weighting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "da77607e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6265734265734266"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly undersample to make adjust class weights\n",
    "\n",
    "neutral_index = (y_train.loc[(y_train == 1)]).sample(356*3, random_state=seed).index\n",
    "positive_index = (y_train.loc[(y_train == 2)]).sample(356*2, random_state=seed).index\n",
    "undersampled_index = np.concatenate((neutral_index, positive_index, (y_train.loc[(y_train == 0)]).index))\n",
    "\n",
    "y_train_undersampled = y_train.loc[undersampled_index].copy()\n",
    "X_train_undersampled = X_train_stems.loc[undersampled_index].copy()\n",
    "\n",
    "final_X_train_undersampled = pd.DataFrame(count.fit_transform(X_train_undersampled).todense(),\n",
    "                                          index=X_train_undersampled.index, columns=count.vocabulary_)\n",
    "final_X_val_us = pd.DataFrame(count.transform(X_val).todense(), index=X_val.index, columns=count.vocabulary_)\n",
    "\n",
    "nb.fit(final_X_train_undersampled, y_train_undersampled)\n",
    "nb.score(final_X_val_us, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66ee4c",
   "metadata": {},
   "source": [
    "Another preprocessing technique we can try is that of term frequency- inverse document frequency vectorization (similar to count vectorization, but adjusted according to how often a word appears in a particular document *versus the corpus as a whole*.)\n",
    "\n",
    "Here are the baseline results with tfidf vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "f7ddbd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "14         stem                          TfidfVectorizer()   \n",
       "15         stem                          TfidfVectorizer()   \n",
       "16         stem                          TfidfVectorizer()   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  "
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "final_X_train = pd.DataFrame(tfidf.fit_transform(X_train_stems).todense(), index=X_train_stems.index,\n",
    "                             columns=tfidf.vocabulary_)\n",
    "final_X_val = pd.DataFrame(tfidf.transform(X_val_stems).todense(), index=X_val_stems.index,\n",
    "                           columns=tfidf.vocabulary_)\n",
    "\n",
    "score_model(nb, final_X_train, y_train, final_X_val, y_val, 'stem', tfidf, nb)\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', tfidf, rf)\n",
    "\n",
    "scaled_X_train = pd.DataFrame(ss.fit_transform(final_X_train), index=final_X_train.index,\n",
    "                              columns=final_X_train.columns)\n",
    "scaled_X_val = pd.DataFrame(ss.transform(final_X_val), index=final_X_val.index,\n",
    "                            columns=final_X_val.columns)\n",
    "\n",
    "score_model(svc, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', tfidf, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dded13a",
   "metadata": {},
   "source": [
    "Let's tune the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "5dd5b2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "14         stem                          TfidfVectorizer()   \n",
       "15         stem                          TfidfVectorizer()   \n",
       "16         stem                          TfidfVectorizer()   \n",
       "17         stem                          TfidfVectorizer()   \n",
       "18         stem                          TfidfVectorizer()   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  "
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed, class_weight='balanced')\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', tfidf, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "fa34e428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "14         stem                          TfidfVectorizer()   \n",
       "15         stem                          TfidfVectorizer()   \n",
       "16         stem                          TfidfVectorizer()   \n",
       "17         stem                          TfidfVectorizer()   \n",
       "18         stem                          TfidfVectorizer()   \n",
       "19         stem                          TfidfVectorizer()   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  "
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed, n_estimators=200)\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', tfidf, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "7ef65b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "14         stem                          TfidfVectorizer()   \n",
       "15         stem                          TfidfVectorizer()   \n",
       "16         stem                          TfidfVectorizer()   \n",
       "17         stem                          TfidfVectorizer()   \n",
       "18         stem                          TfidfVectorizer()   \n",
       "19         stem                          TfidfVectorizer()   \n",
       "20         stem                          TfidfVectorizer()   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  "
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed, max_depth=2)\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', tfidf, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "e3e4c562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "14         stem                          TfidfVectorizer()   \n",
       "15         stem                          TfidfVectorizer()   \n",
       "16         stem                          TfidfVectorizer()   \n",
       "17         stem                          TfidfVectorizer()   \n",
       "18         stem                          TfidfVectorizer()   \n",
       "19         stem                          TfidfVectorizer()   \n",
       "20         stem                          TfidfVectorizer()   \n",
       "21         stem                          TfidfVectorizer()   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  "
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed, max_depth=10)\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', tfidf, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "4008a54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "14         stem                          TfidfVectorizer()   \n",
       "15         stem                          TfidfVectorizer()   \n",
       "16         stem                          TfidfVectorizer()   \n",
       "17         stem                          TfidfVectorizer()   \n",
       "18         stem                          TfidfVectorizer()   \n",
       "19         stem                          TfidfVectorizer()   \n",
       "20         stem                          TfidfVectorizer()   \n",
       "21         stem                          TfidfVectorizer()   \n",
       "22         stem                          TfidfVectorizer()   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  "
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed, max_depth=20)\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', tfidf, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "e74d3765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=100, max_fea...</td>\n",
       "      <td>0.679720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "14         stem                          TfidfVectorizer()   \n",
       "15         stem                          TfidfVectorizer()   \n",
       "16         stem                          TfidfVectorizer()   \n",
       "17         stem                          TfidfVectorizer()   \n",
       "18         stem                          TfidfVectorizer()   \n",
       "19         stem                          TfidfVectorizer()   \n",
       "20         stem                          TfidfVectorizer()   \n",
       "21         stem                          TfidfVectorizer()   \n",
       "22         stem                          TfidfVectorizer()   \n",
       "23         stem                          TfidfVectorizer()   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  \n",
       "23  (DecisionTreeClassifier(max_depth=100, max_fea...  0.679720  "
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed, max_depth=100)\n",
    "\n",
    "score_model(rf, final_X_train, y_train, final_X_val, y_val, 'stem', tfidf, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "3932dfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.14      0.24        78\n",
      "           1       0.68      0.91      0.78       865\n",
      "           2       0.68      0.36      0.47       487\n",
      "\n",
      "    accuracy                           0.68      1430\n",
      "   macro avg       0.70      0.47      0.50      1430\n",
      "weighted avg       0.68      0.68      0.64      1430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf.fit(final_X_train, y_train)\n",
    "\n",
    "print(classification_report(y_val, rf.predict(final_X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "154ab0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    865\n",
       "2    487\n",
       "0     78\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "d04a8462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.85"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "865*0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642f236",
   "metadata": {},
   "source": [
    "Our best classifier so far has a 91% recall of neutral tweets, a 14% recall of negative tweets and a 36% recall of positive tweets. Precision is much higher, with 73% precision for negative tweets, and 68% precision for both neutral and positive tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca742ad",
   "metadata": {},
   "source": [
    "This [paper](http://www.lrec-conf.org/proceedings/lrec2014/pdf/483_Paper.pdf) makes me feel a little better about the results, although the paper was written in 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4b6cd",
   "metadata": {},
   "source": [
    "Another technique for preprocessing is to use word embeddings to vectorize the tweets, to avoid the sparsity seen with count vectorization and tfidf vectorization.\n",
    "\n",
    "I will try using the pretrained GloVe embeddings which were trained on tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "ff50e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "glove_options = ['glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for which_glove in glove_options:\n",
    "    glove = gensim.downloader.load(which_glove)\n",
    "    n_dim = int(which_glove.split('-')[-1]) # number after last dash is the dimensions of the embeddings\n",
    "    final_X_train = glove_embed(X_train_stems, n_dim)\n",
    "    final_X_val = glove_embed(X_val_stems, n_dim)\n",
    "    # using naive bayes as baseline; we need to scale between 0 and 1 to make all features positive (not an issue\n",
    "    # with count or tfidf vectorization, just embeddings)\n",
    "    mms = MinMaxScaler()\n",
    "    scaled_X_train = mms.fit_transform(final_X_train)\n",
    "    scaled_X_val = mms.transform(final_X_val)\n",
    "    score_model(nb, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', which_glove, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "208e4c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=100, max_fea...</td>\n",
       "      <td>0.679720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-25</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-50</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-100</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.605594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "14         stem                          TfidfVectorizer()   \n",
       "15         stem                          TfidfVectorizer()   \n",
       "16         stem                          TfidfVectorizer()   \n",
       "17         stem                          TfidfVectorizer()   \n",
       "18         stem                          TfidfVectorizer()   \n",
       "19         stem                          TfidfVectorizer()   \n",
       "20         stem                          TfidfVectorizer()   \n",
       "21         stem                          TfidfVectorizer()   \n",
       "22         stem                          TfidfVectorizer()   \n",
       "23         stem                          TfidfVectorizer()   \n",
       "24         stem                           glove-twitter-25   \n",
       "25         stem                           glove-twitter-50   \n",
       "26         stem                          glove-twitter-100   \n",
       "27         stem                          glove-twitter-200   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  \n",
       "23  (DecisionTreeClassifier(max_depth=100, max_fea...  0.679720  \n",
       "24                                    MultinomialNB()  0.604895  \n",
       "25                                    MultinomialNB()  0.604895  \n",
       "26                                    MultinomialNB()  0.604895  \n",
       "27                                    MultinomialNB()  0.605594  "
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364f373",
   "metadata": {},
   "source": [
    "Let's give glove a chance with a different type of model - random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "d20a9cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=100, max_fea...</td>\n",
       "      <td>0.679720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-25</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-50</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-100</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.605594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                              preprocessing   \n",
       "0          stem                            count_vectorize  \\\n",
       "1          stem                            count_vectorize   \n",
       "2          stem                            count_vectorize   \n",
       "3     lemmatize                            count_vectorize   \n",
       "4     lemmatize                            count_vectorize   \n",
       "5     lemmatize                            count_vectorize   \n",
       "6          stem   CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem  CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem        CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                          CountVectorizer()   \n",
       "12         stem                          CountVectorizer()   \n",
       "13         stem                          CountVectorizer()   \n",
       "14         stem                          TfidfVectorizer()   \n",
       "15         stem                          TfidfVectorizer()   \n",
       "16         stem                          TfidfVectorizer()   \n",
       "17         stem                          TfidfVectorizer()   \n",
       "18         stem                          TfidfVectorizer()   \n",
       "19         stem                          TfidfVectorizer()   \n",
       "20         stem                          TfidfVectorizer()   \n",
       "21         stem                          TfidfVectorizer()   \n",
       "22         stem                          TfidfVectorizer()   \n",
       "23         stem                          TfidfVectorizer()   \n",
       "24         stem                           glove-twitter-25   \n",
       "25         stem                           glove-twitter-50   \n",
       "26         stem                          glove-twitter-100   \n",
       "27         stem                          glove-twitter-200   \n",
       "28         stem                          glove-twitter-200   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  \n",
       "23  (DecisionTreeClassifier(max_depth=100, max_fea...  0.679720  \n",
       "24                                    MultinomialNB()  0.604895  \n",
       "25                                    MultinomialNB()  0.604895  \n",
       "26                                    MultinomialNB()  0.604895  \n",
       "27                                    MultinomialNB()  0.605594  \n",
       "28  (DecisionTreeClassifier(max_features='sqrt', r...  0.653846  "
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "# last glove embedding was best (200-dimensional) so no need to re-preprocess data\n",
    "score_model(rf, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', which_glove, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b06caf",
   "metadata": {},
   "source": [
    "This may seem like an odd approach, but I wonder if PCA could help our machine learning algorithm to learn from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "9647d83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(scaled_X_train)\n",
    "\n",
    "# how many components must be kept to explain roughly 95% of the variance in the data?\n",
    "np.argmax(np.isclose(np.array([0.95]*200), np.cumsum(pca.explained_variance_ratio_), 0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "46cc5b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=100, max_fea...</td>\n",
       "      <td>0.679720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-25</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-50</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-100</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.605594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>stem</td>\n",
       "      <td>[glove-twitter-200, PCA(n_components=133)]</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                               preprocessing   \n",
       "0          stem                             count_vectorize  \\\n",
       "1          stem                             count_vectorize   \n",
       "2          stem                             count_vectorize   \n",
       "3     lemmatize                             count_vectorize   \n",
       "4     lemmatize                             count_vectorize   \n",
       "5     lemmatize                             count_vectorize   \n",
       "6          stem    CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem   CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem         CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem   CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem         CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                           CountVectorizer()   \n",
       "12         stem                           CountVectorizer()   \n",
       "13         stem                           CountVectorizer()   \n",
       "14         stem                           TfidfVectorizer()   \n",
       "15         stem                           TfidfVectorizer()   \n",
       "16         stem                           TfidfVectorizer()   \n",
       "17         stem                           TfidfVectorizer()   \n",
       "18         stem                           TfidfVectorizer()   \n",
       "19         stem                           TfidfVectorizer()   \n",
       "20         stem                           TfidfVectorizer()   \n",
       "21         stem                           TfidfVectorizer()   \n",
       "22         stem                           TfidfVectorizer()   \n",
       "23         stem                           TfidfVectorizer()   \n",
       "24         stem                            glove-twitter-25   \n",
       "25         stem                            glove-twitter-50   \n",
       "26         stem                           glove-twitter-100   \n",
       "27         stem                           glove-twitter-200   \n",
       "28         stem                           glove-twitter-200   \n",
       "29         stem  [glove-twitter-200, PCA(n_components=133)]   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  \n",
       "23  (DecisionTreeClassifier(max_depth=100, max_fea...  0.679720  \n",
       "24                                    MultinomialNB()  0.604895  \n",
       "25                                    MultinomialNB()  0.604895  \n",
       "26                                    MultinomialNB()  0.604895  \n",
       "27                                    MultinomialNB()  0.605594  \n",
       "28  (DecisionTreeClassifier(max_features='sqrt', r...  0.653846  \n",
       "29  (DecisionTreeClassifier(max_features='sqrt', r...  0.647552  "
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=133)\n",
    "\n",
    "reduced_X_train = pca.fit_transform(scaled_X_train)\n",
    "reduced_X_val = pca.transform(scaled_X_val)\n",
    "\n",
    "score_model(rf, reduced_X_train, y_train, reduced_X_val, y_val, 'stem', [which_glove, pca], rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2357d",
   "metadata": {},
   "source": [
    "Reducing the dimensionality of the glove embeddings artificially didn't have a positive effect on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe19535",
   "metadata": {},
   "source": [
    "One last technique we will try (prior to stacking) is generating our own word embeddings based on the tweets in our corpus, then creating a mean word vector for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "119f2db6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=100, max_fea...</td>\n",
       "      <td>0.679720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-25</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-50</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-100</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.605594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>stem</td>\n",
       "      <td>[glove-twitter-200, PCA(n_components=133)]</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                                      preprocessing   \n",
       "0          stem                                    count_vectorize  \\\n",
       "1          stem                                    count_vectorize   \n",
       "2          stem                                    count_vectorize   \n",
       "3     lemmatize                                    count_vectorize   \n",
       "4     lemmatize                                    count_vectorize   \n",
       "5     lemmatize                                    count_vectorize   \n",
       "6          stem           CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                                  CountVectorizer()   \n",
       "12         stem                                  CountVectorizer()   \n",
       "13         stem                                  CountVectorizer()   \n",
       "14         stem                                  TfidfVectorizer()   \n",
       "15         stem                                  TfidfVectorizer()   \n",
       "16         stem                                  TfidfVectorizer()   \n",
       "17         stem                                  TfidfVectorizer()   \n",
       "18         stem                                  TfidfVectorizer()   \n",
       "19         stem                                  TfidfVectorizer()   \n",
       "20         stem                                  TfidfVectorizer()   \n",
       "21         stem                                  TfidfVectorizer()   \n",
       "22         stem                                  TfidfVectorizer()   \n",
       "23         stem                                  TfidfVectorizer()   \n",
       "24         stem                                   glove-twitter-25   \n",
       "25         stem                                   glove-twitter-50   \n",
       "26         stem                                  glove-twitter-100   \n",
       "27         stem                                  glove-twitter-200   \n",
       "28         stem                                  glove-twitter-200   \n",
       "29         stem         [glove-twitter-200, PCA(n_components=133)]   \n",
       "30         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0....   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  \n",
       "23  (DecisionTreeClassifier(max_depth=100, max_fea...  0.679720  \n",
       "24                                    MultinomialNB()  0.604895  \n",
       "25                                    MultinomialNB()  0.604895  \n",
       "26                                    MultinomialNB()  0.604895  \n",
       "27                                    MultinomialNB()  0.605594  \n",
       "28  (DecisionTreeClassifier(max_features='sqrt', r...  0.653846  \n",
       "29  (DecisionTreeClassifier(max_features='sqrt', r...  0.647552  \n",
       "30                                    MultinomialNB()  0.604895  "
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# try default settings for training first\n",
    "w2v = Word2Vec(X_train_stems.map(lambda x: x.split()))\n",
    "\n",
    "scaled_X_train = mms.fit_transform(custom_embed(X_train_stems, w2v))\n",
    "scaled_X_val = mms.transform(custom_embed(X_val_stems, w2v))\n",
    "\n",
    "score_model(nb, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', w2v, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "64979331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=100, max_fea...</td>\n",
       "      <td>0.679720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-25</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-50</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-100</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.605594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>stem</td>\n",
       "      <td>[glove-twitter-200, PCA(n_components=133)]</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0.01&gt;</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.593007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                                      preprocessing   \n",
       "0          stem                                    count_vectorize  \\\n",
       "1          stem                                    count_vectorize   \n",
       "2          stem                                    count_vectorize   \n",
       "3     lemmatize                                    count_vectorize   \n",
       "4     lemmatize                                    count_vectorize   \n",
       "5     lemmatize                                    count_vectorize   \n",
       "6          stem           CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                                  CountVectorizer()   \n",
       "12         stem                                  CountVectorizer()   \n",
       "13         stem                                  CountVectorizer()   \n",
       "14         stem                                  TfidfVectorizer()   \n",
       "15         stem                                  TfidfVectorizer()   \n",
       "16         stem                                  TfidfVectorizer()   \n",
       "17         stem                                  TfidfVectorizer()   \n",
       "18         stem                                  TfidfVectorizer()   \n",
       "19         stem                                  TfidfVectorizer()   \n",
       "20         stem                                  TfidfVectorizer()   \n",
       "21         stem                                  TfidfVectorizer()   \n",
       "22         stem                                  TfidfVectorizer()   \n",
       "23         stem                                  TfidfVectorizer()   \n",
       "24         stem                                   glove-twitter-25   \n",
       "25         stem                                   glove-twitter-50   \n",
       "26         stem                                  glove-twitter-100   \n",
       "27         stem                                  glove-twitter-200   \n",
       "28         stem                                  glove-twitter-200   \n",
       "29         stem         [glove-twitter-200, PCA(n_components=133)]   \n",
       "30         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0....   \n",
       "31         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0.01>   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  \n",
       "23  (DecisionTreeClassifier(max_depth=100, max_fea...  0.679720  \n",
       "24                                    MultinomialNB()  0.604895  \n",
       "25                                    MultinomialNB()  0.604895  \n",
       "26                                    MultinomialNB()  0.604895  \n",
       "27                                    MultinomialNB()  0.605594  \n",
       "28  (DecisionTreeClassifier(max_features='sqrt', r...  0.653846  \n",
       "29  (DecisionTreeClassifier(max_features='sqrt', r...  0.647552  \n",
       "30                                    MultinomialNB()  0.604895  \n",
       "31                                    MultinomialNB()  0.593007  "
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try varying the settings for training the neural net which produces word embeddings\n",
    "# adjust learning rate\n",
    "w2v = Word2Vec(X_train_stems.map(lambda x: x.split()), alpha=0.01)\n",
    "\n",
    "scaled_X_train = mms.fit_transform(custom_embed(X_train_stems, w2v))\n",
    "scaled_X_val = mms.transform(custom_embed(X_val_stems, w2v))\n",
    "\n",
    "score_model(nb, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', w2v, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "e54484c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=100, max_fea...</td>\n",
       "      <td>0.679720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-25</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-50</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-100</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.605594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>stem</td>\n",
       "      <td>[glove-twitter-200, PCA(n_components=133)]</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0.01&gt;</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.593007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.602797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                                      preprocessing   \n",
       "0          stem                                    count_vectorize  \\\n",
       "1          stem                                    count_vectorize   \n",
       "2          stem                                    count_vectorize   \n",
       "3     lemmatize                                    count_vectorize   \n",
       "4     lemmatize                                    count_vectorize   \n",
       "5     lemmatize                                    count_vectorize   \n",
       "6          stem           CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                                  CountVectorizer()   \n",
       "12         stem                                  CountVectorizer()   \n",
       "13         stem                                  CountVectorizer()   \n",
       "14         stem                                  TfidfVectorizer()   \n",
       "15         stem                                  TfidfVectorizer()   \n",
       "16         stem                                  TfidfVectorizer()   \n",
       "17         stem                                  TfidfVectorizer()   \n",
       "18         stem                                  TfidfVectorizer()   \n",
       "19         stem                                  TfidfVectorizer()   \n",
       "20         stem                                  TfidfVectorizer()   \n",
       "21         stem                                  TfidfVectorizer()   \n",
       "22         stem                                  TfidfVectorizer()   \n",
       "23         stem                                  TfidfVectorizer()   \n",
       "24         stem                                   glove-twitter-25   \n",
       "25         stem                                   glove-twitter-50   \n",
       "26         stem                                  glove-twitter-100   \n",
       "27         stem                                  glove-twitter-200   \n",
       "28         stem                                  glove-twitter-200   \n",
       "29         stem         [glove-twitter-200, PCA(n_components=133)]   \n",
       "30         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0....   \n",
       "31         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0.01>   \n",
       "32         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0....   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  \n",
       "23  (DecisionTreeClassifier(max_depth=100, max_fea...  0.679720  \n",
       "24                                    MultinomialNB()  0.604895  \n",
       "25                                    MultinomialNB()  0.604895  \n",
       "26                                    MultinomialNB()  0.604895  \n",
       "27                                    MultinomialNB()  0.605594  \n",
       "28  (DecisionTreeClassifier(max_features='sqrt', r...  0.653846  \n",
       "29  (DecisionTreeClassifier(max_features='sqrt', r...  0.647552  \n",
       "30                                    MultinomialNB()  0.604895  \n",
       "31                                    MultinomialNB()  0.593007  \n",
       "32                                    MultinomialNB()  0.602797  "
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try varying the settings for training the neural net which produces word embeddings\n",
    "# our documents are short, so try adjusting batch size & window\n",
    "w2v = Word2Vec(X_train_stems.map(lambda x: x.split()), window=3, batch_words=300)\n",
    "\n",
    "scaled_X_train = mms.fit_transform(custom_embed(X_train_stems, w2v))\n",
    "scaled_X_val = mms.transform(custom_embed(X_val_stems, w2v))\n",
    "\n",
    "score_model(nb, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', w2v, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "e20d7334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=100, max_fea...</td>\n",
       "      <td>0.679720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-25</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-50</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-100</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.605594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>stem</td>\n",
       "      <td>[glove-twitter-200, PCA(n_components=133)]</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0.01&gt;</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.593007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.602797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=200, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                                      preprocessing   \n",
       "0          stem                                    count_vectorize  \\\n",
       "1          stem                                    count_vectorize   \n",
       "2          stem                                    count_vectorize   \n",
       "3     lemmatize                                    count_vectorize   \n",
       "4     lemmatize                                    count_vectorize   \n",
       "5     lemmatize                                    count_vectorize   \n",
       "6          stem           CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                                  CountVectorizer()   \n",
       "12         stem                                  CountVectorizer()   \n",
       "13         stem                                  CountVectorizer()   \n",
       "14         stem                                  TfidfVectorizer()   \n",
       "15         stem                                  TfidfVectorizer()   \n",
       "16         stem                                  TfidfVectorizer()   \n",
       "17         stem                                  TfidfVectorizer()   \n",
       "18         stem                                  TfidfVectorizer()   \n",
       "19         stem                                  TfidfVectorizer()   \n",
       "20         stem                                  TfidfVectorizer()   \n",
       "21         stem                                  TfidfVectorizer()   \n",
       "22         stem                                  TfidfVectorizer()   \n",
       "23         stem                                  TfidfVectorizer()   \n",
       "24         stem                                   glove-twitter-25   \n",
       "25         stem                                   glove-twitter-50   \n",
       "26         stem                                  glove-twitter-100   \n",
       "27         stem                                  glove-twitter-200   \n",
       "28         stem                                  glove-twitter-200   \n",
       "29         stem         [glove-twitter-200, PCA(n_components=133)]   \n",
       "30         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0....   \n",
       "31         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0.01>   \n",
       "32         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0....   \n",
       "33         stem  Word2Vec<vocab=1641, vector_size=200, alpha=0....   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  \n",
       "23  (DecisionTreeClassifier(max_depth=100, max_fea...  0.679720  \n",
       "24                                    MultinomialNB()  0.604895  \n",
       "25                                    MultinomialNB()  0.604895  \n",
       "26                                    MultinomialNB()  0.604895  \n",
       "27                                    MultinomialNB()  0.605594  \n",
       "28  (DecisionTreeClassifier(max_features='sqrt', r...  0.653846  \n",
       "29  (DecisionTreeClassifier(max_features='sqrt', r...  0.647552  \n",
       "30                                    MultinomialNB()  0.604895  \n",
       "31                                    MultinomialNB()  0.593007  \n",
       "32                                    MultinomialNB()  0.602797  \n",
       "33                                    MultinomialNB()  0.575524  "
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try varying the settings for training the neural net which produces word embeddings\n",
    "# adjust dimensionality of embeddings\n",
    "w2v = Word2Vec(X_train_stems.map(lambda x: x.split()), vector_size=200)\n",
    "\n",
    "scaled_X_train = mms.fit_transform(custom_embed(X_train_stems, w2v))\n",
    "scaled_X_val = mms.transform(custom_embed(X_val_stems, w2v))\n",
    "\n",
    "score_model(nb, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', w2v, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "52a4a622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenization</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.684615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.653147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.679021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.683916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatize</td>\n",
       "      <td>count_vectorize</td>\n",
       "      <td>support_vector</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.9, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.612587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(max_df=0.95, min_df=0.05)</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2))</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.676923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stem</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.617483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.648252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.673427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>SVC(kernel='poly')</td>\n",
       "      <td>0.616084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.664336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.665035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.671329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=20, max_feat...</td>\n",
       "      <td>0.626573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stem</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=100, max_fea...</td>\n",
       "      <td>0.679720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-25</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-50</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-100</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.605594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stem</td>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>stem</td>\n",
       "      <td>[glove-twitter-200, PCA(n_components=133)]</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0.01&gt;</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.593007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=100, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.602797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=200, alpha=0....</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.575524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>stem</td>\n",
       "      <td>Word2Vec&lt;vocab=1641, vector_size=50, alpha=0.025&gt;</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenization                                      preprocessing   \n",
       "0          stem                                    count_vectorize  \\\n",
       "1          stem                                    count_vectorize   \n",
       "2          stem                                    count_vectorize   \n",
       "3     lemmatize                                    count_vectorize   \n",
       "4     lemmatize                                    count_vectorize   \n",
       "5     lemmatize                                    count_vectorize   \n",
       "6          stem           CountVectorizer(max_df=0.9, min_df=0.05)   \n",
       "7          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "8          stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "9          stem          CountVectorizer(max_df=0.95, min_df=0.05)   \n",
       "10         stem                CountVectorizer(ngram_range=(1, 2))   \n",
       "11         stem                                  CountVectorizer()   \n",
       "12         stem                                  CountVectorizer()   \n",
       "13         stem                                  CountVectorizer()   \n",
       "14         stem                                  TfidfVectorizer()   \n",
       "15         stem                                  TfidfVectorizer()   \n",
       "16         stem                                  TfidfVectorizer()   \n",
       "17         stem                                  TfidfVectorizer()   \n",
       "18         stem                                  TfidfVectorizer()   \n",
       "19         stem                                  TfidfVectorizer()   \n",
       "20         stem                                  TfidfVectorizer()   \n",
       "21         stem                                  TfidfVectorizer()   \n",
       "22         stem                                  TfidfVectorizer()   \n",
       "23         stem                                  TfidfVectorizer()   \n",
       "24         stem                                   glove-twitter-25   \n",
       "25         stem                                   glove-twitter-50   \n",
       "26         stem                                  glove-twitter-100   \n",
       "27         stem                                  glove-twitter-200   \n",
       "28         stem                                  glove-twitter-200   \n",
       "29         stem         [glove-twitter-200, PCA(n_components=133)]   \n",
       "30         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0....   \n",
       "31         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0.01>   \n",
       "32         stem  Word2Vec<vocab=1641, vector_size=100, alpha=0....   \n",
       "33         stem  Word2Vec<vocab=1641, vector_size=200, alpha=0....   \n",
       "34         stem  Word2Vec<vocab=1641, vector_size=50, alpha=0.025>   \n",
       "\n",
       "                                           classifier     score  \n",
       "0                                       random_forest  0.676923  \n",
       "1                                         naive_bayes  0.684615  \n",
       "2                                      support_vector  0.653147  \n",
       "3                                       random_forest  0.679021  \n",
       "4                                         naive_bayes  0.683916  \n",
       "5                                      support_vector  0.647552  \n",
       "6                                     MultinomialNB()  0.612587  \n",
       "7                                     MultinomialNB()  0.612587  \n",
       "8                                     MultinomialNB()  0.690909  \n",
       "9   (DecisionTreeClassifier(max_features='sqrt', r...  0.575524  \n",
       "10  (DecisionTreeClassifier(max_features='sqrt', r...  0.657343  \n",
       "11  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "12  (DecisionTreeClassifier(max_features='sqrt', r...  0.676923  \n",
       "13                                 SVC(kernel='poly')  0.617483  \n",
       "14                                    MultinomialNB()  0.648252  \n",
       "15  (DecisionTreeClassifier(max_features='sqrt', r...  0.673427  \n",
       "16                                 SVC(kernel='poly')  0.616084  \n",
       "17  (DecisionTreeClassifier(max_features='sqrt', r...  0.664336  \n",
       "18  (DecisionTreeClassifier(max_features='sqrt', r...  0.665035  \n",
       "19  (DecisionTreeClassifier(max_features='sqrt', r...  0.671329  \n",
       "20  (DecisionTreeClassifier(max_depth=2, max_featu...  0.604895  \n",
       "21  (DecisionTreeClassifier(max_depth=10, max_feat...  0.607692  \n",
       "22  (DecisionTreeClassifier(max_depth=20, max_feat...  0.626573  \n",
       "23  (DecisionTreeClassifier(max_depth=100, max_fea...  0.679720  \n",
       "24                                    MultinomialNB()  0.604895  \n",
       "25                                    MultinomialNB()  0.604895  \n",
       "26                                    MultinomialNB()  0.604895  \n",
       "27                                    MultinomialNB()  0.605594  \n",
       "28  (DecisionTreeClassifier(max_features='sqrt', r...  0.653846  \n",
       "29  (DecisionTreeClassifier(max_features='sqrt', r...  0.647552  \n",
       "30                                    MultinomialNB()  0.604895  \n",
       "31                                    MultinomialNB()  0.593007  \n",
       "32                                    MultinomialNB()  0.602797  \n",
       "33                                    MultinomialNB()  0.575524  \n",
       "34                                    MultinomialNB()  0.604895  "
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try varying the settings for training the neural net which produces word embeddings\n",
    "# adjust dimensionality of embeddings\n",
    "w2v = Word2Vec(X_train_stems.map(lambda x: x.split()), vector_size=50)\n",
    "\n",
    "scaled_X_train = mms.fit_transform(custom_embed(X_train_stems, w2v))\n",
    "scaled_X_val = mms.transform(custom_embed(X_val_stems, w2v))\n",
    "\n",
    "score_model(nb, scaled_X_train, y_train, scaled_X_val, y_val, 'stem', w2v, nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72324adf",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14d255",
   "metadata": {},
   "source": [
    "The function below (sklearn.model_selection.StratifiedKFold) splits the data into `n_splits` folds, and uses the \"leave one out\" cross-validation methodology to generate the indices of a training and test set within the dataset passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "d34a10df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of train to test is : 3.9982517482517483.\n",
      "The ratio of train to test is : 3.9982517482517483.\n",
      "The ratio of train to test is : 3.9982517482517483.\n",
      "The ratio of train to test is : 4.002624671916011.\n",
      "The ratio of train to test is : 4.002624671916011.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in folds.split(X_train, y_train):\n",
    "    print(f'The ratio of train to test is : {len(X_train.iloc[train_index]) / len(X_train.iloc[test_index])}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3091c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a36cd",
   "metadata": {},
   "source": [
    "When we vectorize the data, it is important to fit the vectorizer only to the training data. I want to apply cross-validation, so I need to split the data into true training and validation sets before fitting the vectorizer or obtaining the embeddings. I will do this with RepeatedStratifiedKFold, with two repeats and only two splits, so there will be four total scores for each model.\n",
    "\n",
    "Additionally, I'd like to give each technique for vectorizing the data a chance with different types of algorithms. Ideally, I would try each technique with every classifier \"type\", but to keep time to a minimum, I will try each preprocessing technique with one of each of the following: Naive Bayes classifier, Random Forest and SVC.\n",
    "\n",
    "We won't have time to tune every combination, so we'll get \"baseline\" scores with each combination, then choose the best combination for each preprocessing technique to tune the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "adc1736b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb</th>\n",
       "      <th>rf</th>\n",
       "      <th>svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.941242</td>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.942082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.941522</td>\n",
       "      <td>0.940123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943201</td>\n",
       "      <td>0.943481</td>\n",
       "      <td>0.941802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.943481</td>\n",
       "      <td>0.940963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941382</td>\n",
       "      <td>0.942571</td>\n",
       "      <td>0.941242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         nb        rf       svc\n",
       "0  0.941242  0.941802  0.942082\n",
       "1  0.941802  0.941522  0.940123\n",
       "2  0.943201  0.943481  0.941802\n",
       "3  0.939284  0.943481  0.940963\n",
       "4  0.941382  0.942571  0.941242"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get scores for count vectorization\n",
    "\n",
    "count_scores = {'nb': [], 'rf': [], 'svc': []}\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "folds = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=seed)\n",
    "\n",
    "for train_index, val_index in folds.split(X_train_lemmas, y_train):\n",
    "    # get validation splits\n",
    "    X_val, y_val = X_train_lemmas.iloc[val_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # get vectorization\n",
    "    count = CountVectorizer()\n",
    "    vectorized_train = pd.DataFrame(count.fit_transform(X_train_lemmas.iloc[train_index]).todense(),\n",
    "                                    index=train_index, columns=count.vocabulary_)\n",
    "    vectorized_val = pd.DataFrame(count.transform(X_val).todense(),\n",
    "                                  index=val_index, columns=count.vocabulary_)\n",
    "    \n",
    "    # scale appropriately for learning algorithm\n",
    "    mms = MinMaxScaler()\n",
    "    scaled_train = mms.fit_transform(vectorized_train)\n",
    "    scaled_val = mms.transform(vectorized_val)\n",
    "    \n",
    "    # classify & append score for this fold\n",
    "    for clf, name in zip([MultinomialNB(), RandomForestClassifier(random_state=seed), SVC(random_state=seed)],\n",
    "                         ['nb', 'rf', 'svc']):\n",
    "        clf.fit(scaled_train, y_train.iloc[train_index])\n",
    "        count_scores[name].append(clf.score(scaled_val, y_val))\n",
    "\n",
    "for clf in ['nb', 'rf', 'svc']:\n",
    "    count_scores[clf].append(np.mean(count_scores[clf]))\n",
    "    \n",
    "pd.DataFrame(count_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8a4cc",
   "metadata": {},
   "source": [
    "Count vectorization, without any tuning, results in a very similar baseline performance for all three classifiers (a little over 94% mean accuracy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3f6c7de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb</th>\n",
       "      <th>rf</th>\n",
       "      <th>svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.939843</td>\n",
       "      <td>0.940403</td>\n",
       "      <td>0.942082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.940123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.940963</td>\n",
       "      <td>0.942921</td>\n",
       "      <td>0.941242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.939843</td>\n",
       "      <td>0.942641</td>\n",
       "      <td>0.941802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.940613</td>\n",
       "      <td>0.941942</td>\n",
       "      <td>0.941312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         nb        rf       svc\n",
       "0  0.939843  0.940403  0.942082\n",
       "1  0.941802  0.941802  0.940123\n",
       "2  0.940963  0.942921  0.941242\n",
       "3  0.939843  0.942641  0.941802\n",
       "4  0.940613  0.941942  0.941312"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get scores for tfidf vectorization\n",
    "\n",
    "tfidf_scores = {'nb': [], 'rf': [], 'svc': []}\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "folds = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=seed)\n",
    "\n",
    "for train_index, val_index in folds.split(X_train_lemmas, y_train):\n",
    "    # get validation splits\n",
    "    X_val, y_val = X_train_lemmas.iloc[val_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # get vectorization\n",
    "    tfidf = TfidfVectorizer()\n",
    "    vectorized_train = pd.DataFrame(tfidf.fit_transform(X_train_lemmas.iloc[train_index]).todense(),\n",
    "                                    index=train_index, columns=tfidf.vocabulary_)\n",
    "    vectorized_val = pd.DataFrame(tfidf.transform(X_val).todense(),\n",
    "                                  index=val_index, columns=tfidf.vocabulary_)\n",
    "    \n",
    "    # scale appropriately for learning algorithm\n",
    "    mms = MinMaxScaler()\n",
    "    scaled_train = mms.fit_transform(vectorized_train)\n",
    "    scaled_val = mms.transform(vectorized_val)\n",
    "    \n",
    "    # classify & append score for this fold\n",
    "    for clf, name in zip([MultinomialNB(), RandomForestClassifier(random_state=seed), SVC(random_state=seed)],\n",
    "                         ['nb', 'rf', 'svc']):\n",
    "        clf.fit(scaled_train, y_train.iloc[train_index])\n",
    "        tfidf_scores[name].append(clf.score(scaled_val, y_val))\n",
    "\n",
    "for clf in ['nb', 'rf', 'svc']:\n",
    "    tfidf_scores[clf].append(np.mean(tfidf_scores[clf]))\n",
    "    \n",
    "pd.DataFrame(tfidf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59bba2a",
   "metadata": {},
   "source": [
    "Tfidf vectorization, wihtout any tuning, performs basically the same as, but slightly worse than, count vectorization for all three classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa788e",
   "metadata": {},
   "source": [
    "To assess the baseline accuracy of several classifiers using pre-trained GloVe embeddings for the words in our tweets, we need to load the GloVe embeddings themselves, define a function to get the \"mean\" word vector for all words in a tweet, and then define a function to transform X_train into a matrix representing the mean word vector for each record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f731fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain full GloVe dictionary\n",
    "\n",
    "with open('data/glove.twitter.27B.50d.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "glove = dict()\n",
    "\n",
    "for line in lines:\n",
    "    items = line.split()\n",
    "    word = items[0]\n",
    "    vector = items[1:]\n",
    "    glove[word] = np.array([float(component) for component in vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00d2a1",
   "metadata": {},
   "source": [
    "Next, define a function to get the mean of the word vectors for all words in a tweet.\n",
    "\n",
    "Based on the average length (in words) of a tweet in this corpus, the function finds the zero-padded mean of the first 21 words in the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_word_vector(tweet):\n",
    "    '''Takes in a string (tweet) and returns the mean word vector of the tweet based on GloVe embeddings and a\n",
    "    normalized tweet \"length\" of 21 words.'''\n",
    "    # hold the GloVe embeddings for each word in the tweet\n",
    "    glove_embeddings = []\n",
    "    \n",
    "    # iterate over each token in the tweet to add its embedding to the list\n",
    "    for token in tweet.split():\n",
    "        try:\n",
    "            glove_embeddings.append(glove[token])\n",
    "        except:\n",
    "            # this token isn't in the GloVe vocab =(\n",
    "            continue\n",
    "    # take the padded mean of the first 21 words in the tweet\n",
    "    if len(glove_embeddings) >= 21:\n",
    "        padded_tweet = glove_embeddings[:21]\n",
    "    elif len(glove_embeddings) >= 1:\n",
    "        padded_tweet = glove_embeddings\n",
    "    else:\n",
    "        padded_tweet = np.array([np.zeros((50,)) for x in range(21)]) # need an array of arrays to output correct type\n",
    "    return np.sum(padded_tweet, axis=0) / 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c715f33",
   "metadata": {},
   "source": [
    "Finally, define a function which transforms a dataframe of tweet data into a dataframe in which each column represents a dimension of the mean word vector of a tweet (based on glove embeddings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "e2db72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embed(X_train, glove_dimensions):\n",
    "    '''Get mean word vector for each tweet in df X_train based on GloVe embeddings.'''\n",
    "    embedded_series = X_train.apply(get_mean_word_vector)\n",
    "    \n",
    "    text_features = {}\n",
    "    \n",
    "    for i in range(glove_dimensions):\n",
    "        feature = f'feature_{i}'\n",
    "        values = [embedding[i] for embedding in embedded_series.values]\n",
    "        text_features[feature] = values\n",
    "    \n",
    "    return pd.DataFrame(text_features, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "85bbbc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb</th>\n",
       "      <th>rf</th>\n",
       "      <th>svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.940123</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939843</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939843</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939773</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         nb        rf       svc\n",
       "0  0.939284  0.940123  0.939284\n",
       "1  0.939284  0.939284  0.939284\n",
       "2  0.939284  0.939843  0.939284\n",
       "3  0.939284  0.939843  0.939284\n",
       "4  0.939284  0.939773  0.939284"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get scores for pre-trained GloVe embeddings\n",
    "\n",
    "glove_scores = {'nb': [], 'rf': [], 'svc': []}\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "folds = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=seed)\n",
    "\n",
    "for train_index, val_index in folds.split(X_train_lemmas, y_train):\n",
    "    # get validation splits\n",
    "    X_val, y_val = X_train_lemmas.iloc[val_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # get vectorization\n",
    "    vectorized_train = glove_embed(X_train_lemmas.iloc[train_index])\n",
    "    vectorized_val = glove_embed(X_val)\n",
    "    \n",
    "    # scale appropriately for learning algorithm\n",
    "    mms = MinMaxScaler()\n",
    "    scaled_train = mms.fit_transform(vectorized_train)\n",
    "    scaled_val = mms.transform(vectorized_val)\n",
    "    \n",
    "    # classify & append score for this fold\n",
    "    for clf, name in zip([MultinomialNB(), RandomForestClassifier(random_state=seed), SVC(random_state=seed)],\n",
    "                         ['nb', 'rf', 'svc']):\n",
    "        clf.fit(scaled_train, y_train.iloc[train_index])\n",
    "        glove_scores[name].append(clf.score(scaled_val, y_val))\n",
    "\n",
    "for clf in ['nb', 'rf', 'svc']:\n",
    "    glove_scores[clf].append(np.mean(glove_scores[clf]))\n",
    "    \n",
    "pd.DataFrame(glove_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964258a6",
   "metadata": {},
   "source": [
    "This cell was so quick! (Comparatively, that is.) The baseline scores for the GloVe representations are close to, but slightly lower than, those of both count and tfidf vectorization. It is strange that the scores are exactly the same for all four folds of both naive bayes and SVC. However, I don't see an error in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71319bf3",
   "metadata": {},
   "source": [
    "Lastly, let's try building a custom Word2Vec embedding for our dataset. First, we define a helper function to get the mean word vector for each tweet (again, assuming a tweet length of 21 words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5f02004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes in a word2vec model, w2v\n",
    "def get_custom_mean_word_vector(tweet, w2v):\n",
    "    '''Takes in a string (tweet) and returns the mean word vector of the tweet based on custom word2vec embeddings\n",
    "    and a normalized tweet \"length\" of 21 words.'''\n",
    "    embeddings = []\n",
    "    for token in tweet.split():\n",
    "        try:\n",
    "            embeddings.append(w2v.wv[token])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # check number of embeddings in tweet and pad or trim as needed\n",
    "    if len(embeddings) >= 21:\n",
    "        padded_tweet = embeddings[:21]\n",
    "    elif len(embeddings) >= 1:\n",
    "        padded_tweet = embeddings\n",
    "    else:\n",
    "        padded_tweet = np.array([np.zeros((w2v.vector_size,)) for x in range(21)])\n",
    "        \n",
    "    return np.sum(padded_tweet, axis=0) / 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6c2b7",
   "metadata": {},
   "source": [
    "Next, we develop a function to feed the embeddings of a Word2Vec model to our helper function above to determine the mean word vector for each tweet in our training data, then produce and return a matrix in which each column corresponds to a dimension of the mean word embedding for that tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "bd3f4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_embed(X_train, w2v):\n",
    "    '''Takes in training data and an instantiated gensim Word2Vec model; returns the matrix form of the training\n",
    "    data based on a mean word vector of length 21 words and the embeddings from Word2Vec model.'''\n",
    "    \n",
    "    # apply helper function to X_train to get mean word vector for each tweet\n",
    "    embedded_series = X_train.apply(get_custom_mean_word_vector, args=(w2v,))\n",
    "    \n",
    "    # convert embeddings to matrix form (each col corresponds to a dimension of the mean word embedding)\n",
    "    text_features = {}\n",
    "    \n",
    "    for i in range(w2v.vector_size):\n",
    "        feature = f'feature_{i}'\n",
    "        values = [embedding[i] for embedding in embedded_series.values]\n",
    "        text_features[feature] = values\n",
    "    \n",
    "    return pd.DataFrame(text_features, index=X_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe528",
   "metadata": {},
   "source": [
    "Lastly, we'll get scores for our three classifiers with our custom word2vec embeddings applied to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a3640a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb</th>\n",
       "      <th>rf</th>\n",
       "      <th>svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.937605</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.938444</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939004</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.937325</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         nb        rf       svc\n",
       "0  0.939284  0.937605  0.939284\n",
       "1  0.939284  0.938444  0.939284\n",
       "2  0.939284  0.939004  0.939284\n",
       "3  0.939284  0.937325  0.939284\n",
       "4  0.939284  0.938095  0.939284"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get scores for custom Word2Vec embeddings\n",
    "\n",
    "w2v_scores = {'nb': [], 'rf': [], 'svc': []}\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "folds = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=seed)\n",
    "\n",
    "for train_index, val_index in folds.split(X_train_lemmas, y_train):\n",
    "    # get validation splits\n",
    "    X_val, y_val = X_train_lemmas.iloc[val_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # get embeddings\n",
    "    w2v = Word2Vec(X_train_lemmas.iloc[train_index].map(lambda x: x.split()),\n",
    "                   vector_size=50, window=5, epochs=5, batch_words=300)\n",
    "    vectorized_train = custom_embed(X_train_lemmas.iloc[train_index], w2v)\n",
    "    vectorized_val = custom_embed(X_val, w2v)\n",
    "    \n",
    "    # scale appropriately for learning algorithm\n",
    "    mms = MinMaxScaler()\n",
    "    scaled_train = mms.fit_transform(vectorized_train)\n",
    "    scaled_val = mms.transform(vectorized_val)\n",
    "    \n",
    "    # classify & append score for this fold\n",
    "    for clf, name in zip([MultinomialNB(), RandomForestClassifier(random_state=seed), SVC(random_state=seed)],\n",
    "                         ['nb', 'rf', 'svc']):\n",
    "        clf.fit(scaled_train, y_train.iloc[train_index])\n",
    "        w2v_scores[name].append(clf.score(scaled_val, y_val))\n",
    "\n",
    "for clf in ['nb', 'rf', 'svc']:\n",
    "    w2v_scores[clf].append(np.mean(w2v_scores[clf]))\n",
    "    \n",
    "pd.DataFrame(w2v_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f33122",
   "metadata": {},
   "source": [
    "The scores for naive bayes and SVC are strangely static for the embedding techniques. This begs an explanation.\n",
    "\n",
    "For now, let's look at all of our scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "3116742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Accuracy Scores on Folds 0-3, Fold 4 is the Mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_nb</th>\n",
       "      <th>count_rf</th>\n",
       "      <th>count_svc</th>\n",
       "      <th>tfidf_nb</th>\n",
       "      <th>tfidf_rf</th>\n",
       "      <th>tfidf_svc</th>\n",
       "      <th>glove_nb</th>\n",
       "      <th>glove_rf</th>\n",
       "      <th>glove_svc</th>\n",
       "      <th>w2v_nb</th>\n",
       "      <th>w2v_rf</th>\n",
       "      <th>w2v_svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.941242</td>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.942082</td>\n",
       "      <td>0.939843</td>\n",
       "      <td>0.940403</td>\n",
       "      <td>0.942082</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.940123</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.937605</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.941522</td>\n",
       "      <td>0.940123</td>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.940123</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.938444</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943201</td>\n",
       "      <td>0.943481</td>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.940963</td>\n",
       "      <td>0.942921</td>\n",
       "      <td>0.941242</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939843</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939004</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.943481</td>\n",
       "      <td>0.940963</td>\n",
       "      <td>0.939843</td>\n",
       "      <td>0.942641</td>\n",
       "      <td>0.941802</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939843</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.937325</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941382</td>\n",
       "      <td>0.942571</td>\n",
       "      <td>0.941242</td>\n",
       "      <td>0.940613</td>\n",
       "      <td>0.941942</td>\n",
       "      <td>0.941312</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939773</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.939284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_nb  count_rf  count_svc  tfidf_nb  tfidf_rf  tfidf_svc  glove_nb   \n",
       "0  0.941242  0.941802   0.942082  0.939843  0.940403   0.942082  0.939284  \\\n",
       "1  0.941802  0.941522   0.940123  0.941802  0.941802   0.940123  0.939284   \n",
       "2  0.943201  0.943481   0.941802  0.940963  0.942921   0.941242  0.939284   \n",
       "3  0.939284  0.943481   0.940963  0.939843  0.942641   0.941802  0.939284   \n",
       "4  0.941382  0.942571   0.941242  0.940613  0.941942   0.941312  0.939284   \n",
       "\n",
       "   glove_rf  glove_svc    w2v_nb    w2v_rf   w2v_svc  \n",
       "0  0.940123   0.939284  0.939284  0.937605  0.939284  \n",
       "1  0.939284   0.939284  0.939284  0.938444  0.939284  \n",
       "2  0.939843   0.939284  0.939284  0.939004  0.939284  \n",
       "3  0.939843   0.939284  0.939284  0.937325  0.939284  \n",
       "4  0.939773   0.939284  0.939284  0.938095  0.939284  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('            Accuracy Scores on Folds 0-3, Fold 4 is the Mean')\n",
    "pd.concat([\n",
    "    pd.DataFrame(count_scores).add_prefix('count_'),\n",
    "    pd.DataFrame(tfidf_scores).add_prefix('tfidf_'),\n",
    "    pd.DataFrame(glove_scores).add_prefix('glove_'),\n",
    "    pd.DataFrame(w2v_scores).add_prefix('w2v_')\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb8f22",
   "metadata": {},
   "source": [
    "Best model for...\n",
    "- Count Vectorization: Random Forest\n",
    "- Tfidf Vectorization: Random Forest\n",
    "- GloVe Embeddings: Random Forest\n",
    "- Word2Vec Embeddings: Tie between Naive Bayes and SVC (possibly affected by weirdly static scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b84ea",
   "metadata": {},
   "source": [
    "So random forest is a clear winner here, except for in the case of Word2Vec.\n",
    "\n",
    "Now we'll try \"tuning\" the preprocessing techniques, with a random forest (except for Word2Vec, which will be paired with SVC.) I'm also going to change the type of scaler since we no longer need Naive Bayes, so I will first get a baseline with Standard Scaler for each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "fbc834db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 6.1% 23.4/387.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 18.2% 70.5/387.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============-----------------------------------] 30.1% 116.7/387.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================------------------------------] 41.9% 162.2/387.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================------------------------] 53.7% 208.0/387.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================-----------------] 66.2% 256.2/387.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================--------] 84.3% 326.5/387.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 96.1% 372.1/387.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[=-------------------------------------------------] 3.1% 23.4/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====----------------------------------------------] 8.8% 67.0/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======-------------------------------------------] 15.0% 113.7/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========----------------------------------------] 21.3% 161.5/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============-------------------------------------] 27.4% 207.8/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================----------------------------------] 33.8% 256.3/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================------------------------------] 40.2% 304.7/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================---------------------------] 46.5% 353.0/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================------------------------] 53.2% 403.8/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 60.3% 457.6/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 68.3% 518.4/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================================-------------] 75.0% 568.8/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================================---------] 83.5% 633.5/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 90.7% 688.2/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.7% 756.2/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[324], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m all_options \u001b[38;5;241m=\u001b[39m count_options \u001b[38;5;241m+\u001b[39m tfidf_options \u001b[38;5;241m+\u001b[39m glove_options \u001b[38;5;241m+\u001b[39m w2v_options\n\u001b[1;32m    113\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m count_scores \u001b[38;5;241m+\u001b[39m tfidf_scores \u001b[38;5;241m+\u001b[39m glove_scores \u001b[38;5;241m+\u001b[39m w2v_scores\n\u001b[0;32m--> 115\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing_technique\u001b[39m\u001b[38;5;124m'\u001b[39m: all_options, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross_val_score\u001b[39m\u001b[38;5;124m'\u001b[39m: all_scores})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# \"tune\" preprocessing techniques\n",
    "\n",
    "# get folds\n",
    "folds = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=seed)\n",
    "\n",
    "# instantiate classifiers\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "svc = SVC(random_state=seed)\n",
    "\n",
    "# instantiate standard scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# options for count vectorizer\n",
    "count_options = [CountVectorizer(), CountVectorizer(min_df=0.05, max_df=0.95), CountVectorizer(max_features=500),\n",
    "                 CountVectorizer(ngram_range=(1, 2)), CountVectorizer(ngram_range=(1, 2), min_df=0.05, max_df=0.95),\n",
    "                 CountVectorizer(ngram_range=(1, 2), max_features=500)]\n",
    "count_scores = []\n",
    "\n",
    "# options for tfidf vectorizer\n",
    "tfidf_options = [TfidfVectorizer(), TfidfVectorizer(min_df=0.05, max_df=0.95), TfidfVectorizer(max_features=500),\n",
    "                 TfidfVectorizer(ngram_range=(1, 2)), TfidfVectorizer(ngram_range=(1, 2), min_df=0.05, max_df=0.95),\n",
    "                 TfidfVectorizer(ngram_range=(1, 2), max_features=500)]\n",
    "tfidf_scores = []\n",
    "\n",
    "# options for glove embeddings\n",
    "import gensim.downloader\n",
    "glove_options = ['glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200']\n",
    "glove_scores = []\n",
    "\n",
    "# options for custom embeddings\n",
    "w2v_options = []\n",
    "# iterate over individual settings\n",
    "vector_size_options = [25, 50, 100, 200]\n",
    "window_options = [3, 5]\n",
    "min_count_options = [1, 5, 10]\n",
    "max_vocab_size_options = [30000, 500] # None seems to be a problem, so use 30000 for \"no max\"\n",
    "epochs = 10\n",
    "batch_words = 300\n",
    "\n",
    "for vector_size in vector_size_options:\n",
    "    for window in window_options:\n",
    "        for min_count in min_count_options:\n",
    "            for max_vocab_size in max_vocab_size_options:\n",
    "                w2v_options.append({'vector_size': vector_size, 'window': window, 'min_count': min_count,\n",
    "                                    'max_vocab_size': max_vocab_size, 'epochs': epochs, 'batch_words': batch_words})    \n",
    "w2v_scores = []\n",
    "\n",
    "for count, tfidf, which_glove, w2v in zip(count_options, tfidf_options, glove_options, w2v_options):\n",
    "    # collect score for each fold\n",
    "    count_cross_val_scores = []\n",
    "    tfidf_cross_val_scores = []\n",
    "    glove_cross_val_scores = []\n",
    "    w2v_cross_val_scores = []\n",
    "    \n",
    "    for train_index, val_index in folds.split(X_train_lemmas, y_train):\n",
    "        \n",
    "        # get validation splits\n",
    "        X_val, y_val = X_train_lemmas.iloc[val_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # fit count vectorizer & scaler; transform training & validation data\n",
    "        preprocessed_train = ss.fit_transform(\n",
    "            pd.DataFrame(count.fit_transform(X_train_lemmas.iloc[train_index]).todense())\n",
    "        )\n",
    "        preprocessed_val = ss.transform(\n",
    "            pd.DataFrame(count.transform(X_val).todense())\n",
    "        )\n",
    "        # SCORE\n",
    "        # fit classifier to training data; score on validation data\n",
    "        rf.fit(preprocessed_train, y_train.iloc[train_index])\n",
    "        count_cross_val_scores.append(rf.score(preprocessed_val, y_val))\n",
    "        \n",
    "        # fit tfidf vectorizer & scaler; transform training & validation data\n",
    "        preprocessed_train = ss.fit_transform(\n",
    "            pd.DataFrame(tfidf.fit_transform(X_train_lemmas.iloc[train_index]).todense())\n",
    "        )\n",
    "        preprocessed_val = ss.transform(\n",
    "            pd.DataFrame(tfidf.transform(X_val).todense())\n",
    "        )\n",
    "        # SCORE\n",
    "        # fit classifier to training data; score on validation data\n",
    "        rf.fit(preprocessed_train, y_train.iloc[train_index])\n",
    "        tfidf_cross_val_scores.append(rf.score(preprocessed_val, y_val))\n",
    "        \n",
    "        # download appropriate glove embeddings; embed & scale training & validation data\n",
    "        glove = gensim.downloader.load(which_glove)\n",
    "        n_dim = int(which_glove.split('-')[-1])\n",
    "        preprocessed_train = ss.fit_transform(glove_embed(X_train_lemmas.iloc[train_index], n_dim))\n",
    "        preprocessed_val = ss.transform(glove_embed(X_val, n_dim))\n",
    "        # SCORE\n",
    "        # fit classifier to training data; score on validation data\n",
    "        rf.fit(preprocessed_train, y_train.iloc[train_index])\n",
    "        glove_cross_val_scores.append(rf.score(preprocessed_val, y_val))\n",
    "        \n",
    "        # build Word2Vec model using training data & appropriate settings; embed & scale train & validation data\n",
    "        w2v_data = X_train_lemmas.iloc[train_index].map(lambda x: x.split())\n",
    "        w2v_model = Word2Vec(w2v_data, vector_size=w2v['vector_size'], window=w2v['window'],\n",
    "                             min_count=w2v['min_count'], max_vocab_size=w2v['max_vocab_size'],\n",
    "                             epochs=w2v['epochs'], batch_words=w2v['batch_words'])\n",
    "        \n",
    "        preprocessed_train = ss.fit_transform(custom_embed(X_train_lemmas.iloc[train_index], w2v_model))\n",
    "        preprocessed_val = ss.transform(custom_embed(X_val, w2v_model))\n",
    "        # SCORE\n",
    "        # fit classifier to training data; score on validation data\n",
    "        svc.fit(preprocessed_train, y_train.iloc[train_index])\n",
    "        w2v_cross_val_scores.append(svc.score(preprocessed_val, y_val))\n",
    "        \n",
    "    count_scores.append(np.mean(count_cross_val_scores))\n",
    "    tfidf_scores.append(np.mean(tfidf_cross_val_scores))\n",
    "    glove_scores.append(np.mean(glove_cross_val_scores))\n",
    "    w2v_scores.append(np.mean(w2v_cross_val_scores))\n",
    "        \n",
    "all_options = count_options + tfidf_options + glove_options + w2v_options\n",
    "all_scores = count_scores + tfidf_scores + glove_scores + w2v_scores\n",
    "\n",
    "pd.DataFrame({'preprocessing_technique': all_options, 'cross_val_score': all_scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b5903",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e27e2",
   "metadata": {},
   "source": [
    "1. Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "afa10380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>next</th>\n",
       "      <th>sxsw</th>\n",
       "      <th>we</th>\n",
       "      <th>re</th>\n",
       "      <th>heading</th>\n",
       "      <th>to</th>\n",
       "      <th>is</th>\n",
       "      <th>about</th>\n",
       "      <th>designing</th>\n",
       "      <th>ipad</th>\n",
       "      <th>...</th>\n",
       "      <th>tomato</th>\n",
       "      <th>mat</th>\n",
       "      <th>scarfing</th>\n",
       "      <th>fab</th>\n",
       "      <th>awe</th>\n",
       "      <th>jetsons</th>\n",
       "      <th>atl</th>\n",
       "      <th>chugging</th>\n",
       "      <th>prepped</th>\n",
       "      <th>underwire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      next  sxsw  we  re  heading  to  is  about  designing  ipad  ...   \n",
       "8841     0     0   0   0        0   0   0      0          0     0  ...  \\\n",
       "1099     0     0   0   0        0   0   0      0          0     0  ...   \n",
       "3738     0     0   0   0        0   0   0      0          0     0  ...   \n",
       "4721     0     0   0   0        0   0   0      0          0     0  ...   \n",
       "4200     0     0   0   0        0   0   0      0          0     0  ...   \n",
       "\n",
       "      tomato  mat  scarfing  fab  awe  jetsons  atl  chugging  prepped   \n",
       "8841       0    0         0    0    0        0    0         0        0  \\\n",
       "1099       0    0         0    0    0        0    0         0        0   \n",
       "3738       0    0         0    0    0        0    0         0        0   \n",
       "4721       0    0         0    0    0        0    0         0        0   \n",
       "4200       0    0         0    0    0        0    0         0        0   \n",
       "\n",
       "      underwire  \n",
       "8841          0  \n",
       "1099          0  \n",
       "3738          0  \n",
       "4721          0  \n",
       "4200          0  \n",
       "\n",
       "[5 rows x 8777 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def count_dense(X_train):\n",
    "    count = CountVectorizer()\n",
    "    return pd.DataFrame(count.fit_transform(X_train).todense(), index=X_train.index, columns=count.vocabulary_)\n",
    "\n",
    "count_transformer = FunctionTransformer(count_dense)\n",
    "count_transformer.fit_transform(X_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "90118ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>next</th>\n",
       "      <th>sxsw</th>\n",
       "      <th>we</th>\n",
       "      <th>re</th>\n",
       "      <th>heading</th>\n",
       "      <th>to</th>\n",
       "      <th>is</th>\n",
       "      <th>about</th>\n",
       "      <th>designing</th>\n",
       "      <th>ipad</th>\n",
       "      <th>...</th>\n",
       "      <th>tomato</th>\n",
       "      <th>mat</th>\n",
       "      <th>scarfing</th>\n",
       "      <th>fab</th>\n",
       "      <th>awe</th>\n",
       "      <th>jetsons</th>\n",
       "      <th>atl</th>\n",
       "      <th>chugging</th>\n",
       "      <th>prepped</th>\n",
       "      <th>underwire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      next  sxsw   we   re  heading   to   is  about  designing  ipad  ...   \n",
       "8841   0.0   0.0  0.0  0.0      0.0  0.0  0.0    0.0        0.0   0.0  ...  \\\n",
       "1099   0.0   0.0  0.0  0.0      0.0  0.0  0.0    0.0        0.0   0.0  ...   \n",
       "3738   0.0   0.0  0.0  0.0      0.0  0.0  0.0    0.0        0.0   0.0  ...   \n",
       "4721   0.0   0.0  0.0  0.0      0.0  0.0  0.0    0.0        0.0   0.0  ...   \n",
       "4200   0.0   0.0  0.0  0.0      0.0  0.0  0.0    0.0        0.0   0.0  ...   \n",
       "\n",
       "      tomato  mat  scarfing  fab  awe  jetsons  atl  chugging  prepped   \n",
       "8841     0.0  0.0       0.0  0.0  0.0      0.0  0.0       0.0      0.0  \\\n",
       "1099     0.0  0.0       0.0  0.0  0.0      0.0  0.0       0.0      0.0   \n",
       "3738     0.0  0.0       0.0  0.0  0.0      0.0  0.0       0.0      0.0   \n",
       "4721     0.0  0.0       0.0  0.0  0.0      0.0  0.0       0.0      0.0   \n",
       "4200     0.0  0.0       0.0  0.0  0.0      0.0  0.0       0.0      0.0   \n",
       "\n",
       "      underwire  \n",
       "8841        0.0  \n",
       "1099        0.0  \n",
       "3738        0.0  \n",
       "4721        0.0  \n",
       "4200        0.0  \n",
       "\n",
       "[5 rows x 8777 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "def tfidf_dense(X_train):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    return pd.DataFrame(tfidf.fit_transform(X_train).todense(), index=X_train.index, columns=tfidf.vocabulary_)\n",
    "\n",
    "tfidf_transformer = FunctionTransformer(tfidf_dense)\n",
    "\n",
    "tfidf_transformer.fit_transform(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c63b57",
   "metadata": {},
   "source": [
    "Count Vectorization / Tfidf Vectorization\n",
    "\n",
    "Tune:\n",
    "- mindf\n",
    "- maxdf\n",
    "- max_features\n",
    "- ngram_range (including bigrams, trigrams, etc.)\n",
    "\n",
    "-- norm (for tfidf) could be l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "94d8c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4f5b6",
   "metadata": {},
   "source": [
    "2. Pretrained GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7522f5f",
   "metadata": {},
   "source": [
    "Pretrained GloVe vectors\n",
    "\n",
    "Tune:\n",
    "\n",
    "- Dimensions of embeddings\n",
    "\n",
    "(To save time on training / tuning, apply once to all of X_train, and save to file; then load in the appropriate dataframe & use GloVe train when needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6fc815f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain full GloVe dictionary\n",
    "\n",
    "with open('data/glove.twitter.27B.50d.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "glove = dict()\n",
    "\n",
    "for line in lines:\n",
    "    items = line.split()\n",
    "    word = items[0]\n",
    "    vector = items[1:]\n",
    "    glove[word] = np.array([float(component) for component in vector])\n",
    "\n",
    "# define function to get mean word vector for a tweet    \n",
    "\n",
    "def get_mean_word_vector(tweet):\n",
    "    '''Takes in a string (tweet) and returns the mean word vector of the tweet based on GloVe embeddings and a\n",
    "    normalized tweet \"length\" of 21 words.'''\n",
    "    # hold the GloVe embeddings for each word in the tweet\n",
    "    glove_embeddings = []\n",
    "    \n",
    "    # iterate over each token in the tweet to add its embedding to the list\n",
    "    for token in tweet.split():\n",
    "        try:\n",
    "            glove_embeddings.append(glove[token])\n",
    "        except:\n",
    "            # this token isn't in the GloVe vocab =(\n",
    "            continue\n",
    "    # take the padded mean of the first 21 words in the tweet\n",
    "    if len(glove_embeddings) >= 21:\n",
    "        padded_tweet = glove_embeddings[:21]\n",
    "    elif len(glove_embeddings) >= 1:\n",
    "        padded_tweet = glove_embeddings\n",
    "    else:\n",
    "        padded_tweet = np.array([np.zeros((50,)) for x in range(21)]) # need an array of arrays to output correct type\n",
    "    return np.sum(padded_tweet, axis=0) / 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3f01bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embed(X_train):\n",
    "    '''Get mean word vector for each tweet in df X_train based on GloVe embeddings.'''\n",
    "    embedded_series = X_train.apply(get_mean_word_vector)\n",
    "    \n",
    "    text_features = {}\n",
    "    \n",
    "    for i in range(50):\n",
    "        feature = f'feature_{i}'\n",
    "        values = [embedding[i] for embedding in embedded_series.values]\n",
    "        text_features[feature] = values\n",
    "    \n",
    "    return pd.DataFrame(text_features, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c587b529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>0.207180</td>\n",
       "      <td>0.302964</td>\n",
       "      <td>-0.035057</td>\n",
       "      <td>-0.066851</td>\n",
       "      <td>-0.074102</td>\n",
       "      <td>0.100418</td>\n",
       "      <td>0.557413</td>\n",
       "      <td>-0.086085</td>\n",
       "      <td>0.091755</td>\n",
       "      <td>-0.125655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.633701</td>\n",
       "      <td>0.167626</td>\n",
       "      <td>0.142875</td>\n",
       "      <td>-0.015301</td>\n",
       "      <td>0.049324</td>\n",
       "      <td>-0.262356</td>\n",
       "      <td>-0.076559</td>\n",
       "      <td>0.018596</td>\n",
       "      <td>-0.181829</td>\n",
       "      <td>0.024528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>0.179931</td>\n",
       "      <td>0.094233</td>\n",
       "      <td>0.016722</td>\n",
       "      <td>-0.157743</td>\n",
       "      <td>-0.047517</td>\n",
       "      <td>0.047872</td>\n",
       "      <td>0.287601</td>\n",
       "      <td>-0.131410</td>\n",
       "      <td>-0.047775</td>\n",
       "      <td>-0.333343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.369052</td>\n",
       "      <td>0.300042</td>\n",
       "      <td>0.218136</td>\n",
       "      <td>0.053288</td>\n",
       "      <td>0.149375</td>\n",
       "      <td>0.011477</td>\n",
       "      <td>-0.139160</td>\n",
       "      <td>-0.029346</td>\n",
       "      <td>0.033697</td>\n",
       "      <td>0.136504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.049583</td>\n",
       "      <td>-0.018715</td>\n",
       "      <td>-0.010397</td>\n",
       "      <td>-0.004129</td>\n",
       "      <td>0.036033</td>\n",
       "      <td>0.127526</td>\n",
       "      <td>-0.014300</td>\n",
       "      <td>0.032982</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122343</td>\n",
       "      <td>0.034782</td>\n",
       "      <td>0.034259</td>\n",
       "      <td>0.017119</td>\n",
       "      <td>0.108324</td>\n",
       "      <td>-0.078668</td>\n",
       "      <td>0.047707</td>\n",
       "      <td>-0.005141</td>\n",
       "      <td>-0.058254</td>\n",
       "      <td>-0.005824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>0.083003</td>\n",
       "      <td>0.102894</td>\n",
       "      <td>-0.012296</td>\n",
       "      <td>-0.037146</td>\n",
       "      <td>-0.035374</td>\n",
       "      <td>-0.034826</td>\n",
       "      <td>0.155753</td>\n",
       "      <td>0.026458</td>\n",
       "      <td>0.047580</td>\n",
       "      <td>0.127528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126584</td>\n",
       "      <td>0.077932</td>\n",
       "      <td>0.081088</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>-0.080054</td>\n",
       "      <td>0.038435</td>\n",
       "      <td>-0.043624</td>\n",
       "      <td>-0.015555</td>\n",
       "      <td>0.028616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>-0.001186</td>\n",
       "      <td>0.112821</td>\n",
       "      <td>-0.058175</td>\n",
       "      <td>0.063682</td>\n",
       "      <td>-0.065543</td>\n",
       "      <td>0.075182</td>\n",
       "      <td>0.389694</td>\n",
       "      <td>-0.113389</td>\n",
       "      <td>0.075396</td>\n",
       "      <td>-0.114265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710728</td>\n",
       "      <td>0.086917</td>\n",
       "      <td>-0.115072</td>\n",
       "      <td>-0.063434</td>\n",
       "      <td>0.266018</td>\n",
       "      <td>-0.064404</td>\n",
       "      <td>0.085022</td>\n",
       "      <td>0.036877</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>0.102232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5   \n",
       "8841   0.207180   0.302964  -0.035057  -0.066851  -0.074102   0.100418  \\\n",
       "1099   0.179931   0.094233   0.016722  -0.157743  -0.047517   0.047872   \n",
       "3738   0.006968   0.049583  -0.018715  -0.010397  -0.004129   0.036033   \n",
       "4721   0.083003   0.102894  -0.012296  -0.037146  -0.035374  -0.034826   \n",
       "4200  -0.001186   0.112821  -0.058175   0.063682  -0.065543   0.075182   \n",
       "\n",
       "      feature_6  feature_7  feature_8  feature_9  ...  feature_40  feature_41   \n",
       "8841   0.557413  -0.086085   0.091755  -0.125655  ...   -0.633701    0.167626  \\\n",
       "1099   0.287601  -0.131410  -0.047775  -0.333343  ...   -0.369052    0.300042   \n",
       "3738   0.127526  -0.014300   0.032982   0.070221  ...   -0.122343    0.034782   \n",
       "4721   0.155753   0.026458   0.047580   0.127528  ...   -0.126584    0.077932   \n",
       "4200   0.389694  -0.113389   0.075396  -0.114265  ...   -0.710728    0.086917   \n",
       "\n",
       "      feature_42  feature_43  feature_44  feature_45  feature_46  feature_47   \n",
       "8841    0.142875   -0.015301    0.049324   -0.262356   -0.076559    0.018596  \\\n",
       "1099    0.218136    0.053288    0.149375    0.011477   -0.139160   -0.029346   \n",
       "3738    0.034259    0.017119    0.108324   -0.078668    0.047707   -0.005141   \n",
       "4721    0.081088    0.066120    0.071275   -0.080054    0.038435   -0.043624   \n",
       "4200   -0.115072   -0.063434    0.266018   -0.064404    0.085022    0.036877   \n",
       "\n",
       "      feature_48  feature_49  \n",
       "8841   -0.181829    0.024528  \n",
       "1099    0.033697    0.136504  \n",
       "3738   -0.058254   -0.005824  \n",
       "4721   -0.015555    0.028616  \n",
       "4200    0.024176    0.102232  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "glove_transformer = FunctionTransformer(glove_embed)\n",
    "\n",
    "glove_transformer.transform(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1e2d3",
   "metadata": {},
   "source": [
    "Word2Vec Embeddings\n",
    "\n",
    "Tune:\n",
    "\n",
    "- Dimensions of embeddings\n",
    "- window (context for learning embeddings)\n",
    "- min count (drop words that are less frequent)\n",
    "- alpha (learning rate)\n",
    "- min alpha (learning rate drops to this value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_mean_word_vector(tweet, w2v):\n",
    "    '''Takes in a string (tweet) and returns the mean word vector of the tweet based on custom word2vec embeddings\n",
    "    and a normalized tweet \"length\" of 21 words.'''\n",
    "    embeddings = []\n",
    "    for token in tweet.split():\n",
    "        try:\n",
    "            embeddings.append(w2v.wv[token])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # check number of embeddings in tweet and pad or trim as needed\n",
    "    if len(embeddings) >= 21:\n",
    "        padded_tweet = embeddings[:21]\n",
    "    elif len(embeddings) >= 1:\n",
    "        padded_tweet = embeddings\n",
    "    else:\n",
    "        padded_tweet = np.array([np.zeros((w2v.vector_size,)) for x in range(21)])\n",
    "        \n",
    "    return np.sum(padded_tweet, axis=0) / 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "68c2f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def custom_embed(X_train, n_dim=50, window_size=5, n_epochs=5, batch_size=300):\n",
    "    '''Builds a custom word2vec embedding based on training data, then finds mean word vector for each tweet.'''\n",
    "    w2v = Word2Vec(X_train.map(lambda x: x.split()), vector_size=n_dim, window=window_size, epochs=n_epochs,\n",
    "                   batch_words=batch_size)\n",
    "    \n",
    "    embedded_series = X_train.apply(get_custom_mean_word_vector, args=(w2v,))\n",
    "    \n",
    "    text_features = {}\n",
    "    \n",
    "    for i in range(n_dim):\n",
    "        feature = f'feature_{i}'\n",
    "        values = [embedding[i] for embedding in embedded_series.values]\n",
    "        text_features[feature] = values\n",
    "    \n",
    "    return pd.DataFrame(text_features, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "608a34ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>0.268304</td>\n",
       "      <td>-0.387301</td>\n",
       "      <td>-0.160716</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>-0.083303</td>\n",
       "      <td>-0.313982</td>\n",
       "      <td>0.507078</td>\n",
       "      <td>0.670895</td>\n",
       "      <td>-0.734434</td>\n",
       "      <td>-0.356796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491259</td>\n",
       "      <td>-0.235175</td>\n",
       "      <td>-0.148520</td>\n",
       "      <td>0.136784</td>\n",
       "      <td>1.093976</td>\n",
       "      <td>0.345862</td>\n",
       "      <td>-0.100481</td>\n",
       "      <td>-0.415576</td>\n",
       "      <td>0.181123</td>\n",
       "      <td>0.172490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>0.143718</td>\n",
       "      <td>-0.236713</td>\n",
       "      <td>-0.140643</td>\n",
       "      <td>-0.031581</td>\n",
       "      <td>-0.049980</td>\n",
       "      <td>-0.238485</td>\n",
       "      <td>0.393407</td>\n",
       "      <td>0.476965</td>\n",
       "      <td>-0.528979</td>\n",
       "      <td>-0.213734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331534</td>\n",
       "      <td>-0.134465</td>\n",
       "      <td>-0.083678</td>\n",
       "      <td>0.097264</td>\n",
       "      <td>0.815410</td>\n",
       "      <td>0.198341</td>\n",
       "      <td>-0.058686</td>\n",
       "      <td>-0.340304</td>\n",
       "      <td>0.140526</td>\n",
       "      <td>0.080673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>0.163452</td>\n",
       "      <td>-0.150679</td>\n",
       "      <td>-0.041837</td>\n",
       "      <td>-0.007195</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>-0.048097</td>\n",
       "      <td>0.215699</td>\n",
       "      <td>0.271207</td>\n",
       "      <td>-0.262472</td>\n",
       "      <td>-0.145882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182345</td>\n",
       "      <td>-0.127320</td>\n",
       "      <td>-0.127885</td>\n",
       "      <td>0.113856</td>\n",
       "      <td>0.471990</td>\n",
       "      <td>0.188884</td>\n",
       "      <td>-0.024240</td>\n",
       "      <td>-0.160826</td>\n",
       "      <td>0.080084</td>\n",
       "      <td>0.036271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>0.125279</td>\n",
       "      <td>-0.126908</td>\n",
       "      <td>-0.089278</td>\n",
       "      <td>-0.036047</td>\n",
       "      <td>0.015444</td>\n",
       "      <td>-0.128214</td>\n",
       "      <td>0.317593</td>\n",
       "      <td>0.308656</td>\n",
       "      <td>-0.347144</td>\n",
       "      <td>-0.131269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213070</td>\n",
       "      <td>-0.109464</td>\n",
       "      <td>-0.107233</td>\n",
       "      <td>0.080241</td>\n",
       "      <td>0.611678</td>\n",
       "      <td>0.131026</td>\n",
       "      <td>-0.034458</td>\n",
       "      <td>-0.212015</td>\n",
       "      <td>0.134513</td>\n",
       "      <td>0.009610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.229077</td>\n",
       "      <td>-0.366446</td>\n",
       "      <td>-0.145863</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>-0.100780</td>\n",
       "      <td>-0.310617</td>\n",
       "      <td>0.538120</td>\n",
       "      <td>0.681079</td>\n",
       "      <td>-0.762059</td>\n",
       "      <td>-0.414936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539044</td>\n",
       "      <td>-0.250698</td>\n",
       "      <td>-0.168525</td>\n",
       "      <td>0.126220</td>\n",
       "      <td>1.212521</td>\n",
       "      <td>0.285830</td>\n",
       "      <td>-0.084045</td>\n",
       "      <td>-0.452978</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>0.164887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5   \n",
       "8841   0.268304  -0.387301  -0.160716   0.004816  -0.083303  -0.313982  \\\n",
       "1099   0.143718  -0.236713  -0.140643  -0.031581  -0.049980  -0.238485   \n",
       "3738   0.163452  -0.150679  -0.041837  -0.007195   0.001633  -0.048097   \n",
       "4721   0.125279  -0.126908  -0.089278  -0.036047   0.015444  -0.128214   \n",
       "4200   0.229077  -0.366446  -0.145863   0.021870  -0.100780  -0.310617   \n",
       "\n",
       "      feature_6  feature_7  feature_8  feature_9  ...  feature_40  feature_41   \n",
       "8841   0.507078   0.670895  -0.734434  -0.356796  ...    0.491259   -0.235175  \\\n",
       "1099   0.393407   0.476965  -0.528979  -0.213734  ...    0.331534   -0.134465   \n",
       "3738   0.215699   0.271207  -0.262472  -0.145882  ...    0.182345   -0.127320   \n",
       "4721   0.317593   0.308656  -0.347144  -0.131269  ...    0.213070   -0.109464   \n",
       "4200   0.538120   0.681079  -0.762059  -0.414936  ...    0.539044   -0.250698   \n",
       "\n",
       "      feature_42  feature_43  feature_44  feature_45  feature_46  feature_47   \n",
       "8841   -0.148520    0.136784    1.093976    0.345862   -0.100481   -0.415576  \\\n",
       "1099   -0.083678    0.097264    0.815410    0.198341   -0.058686   -0.340304   \n",
       "3738   -0.127885    0.113856    0.471990    0.188884   -0.024240   -0.160826   \n",
       "4721   -0.107233    0.080241    0.611678    0.131026   -0.034458   -0.212015   \n",
       "4200   -0.168525    0.126220    1.212521    0.285830   -0.084045   -0.452978   \n",
       "\n",
       "      feature_48  feature_49  \n",
       "8841    0.181123    0.172490  \n",
       "1099    0.140526    0.080673  \n",
       "3738    0.080084    0.036271  \n",
       "4721    0.134513    0.009610  \n",
       "4200    0.204800    0.164887  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "word2vec_transformer = FunctionTransformer(custom_embed, kw_args={'window_size': 5, 'n_epochs': 5})\n",
    "\n",
    "word2vec_transformer.transform(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d1344",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c064b",
   "metadata": {},
   "source": [
    "<u>Overview</u>\n",
    "\n",
    "Build a pipeline for each preprocessing technique.\n",
    "\n",
    "Iterate over several classifier types (baseline model).\n",
    "\n",
    "Select best performing combination for each preprocessing technique to TUNE.\n",
    "\n",
    "- Round 1 tuning: tune preprocessing techniques.\n",
    "- Round 2 tuning: tune classifier.\n",
    "- Round 3 tuning (only for best performance so far): tune preprocessing again.\n",
    "\n",
    "Select best performing preprocessing/classifier combination to THRESHOLD.\n",
    "\n",
    "Threshold best classifier.\n",
    "\n",
    "Evaluate best model by doing the following:\n",
    "\n",
    "- fit on full training data\n",
    "- get model score(s) on hold out test set\n",
    "- print confusion matrices for hold out test set\n",
    "\n",
    "Write a written summary on model performance.\n",
    "\n",
    "Log next steps to be taken.\n",
    "\n",
    "Write README based on work so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0130eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # long\n",
    "# add xgboost: from xgboost import XGBoostClassifier after conda install py-xgboost\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7cd32136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store scores for each combination of options\n",
    "\n",
    "results = {'tokenization_method': [], 'vectorization_method': [], 'classifier': [], 'score': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "831c8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store names and data / transformers / estimators needed\n",
    "\n",
    "tokenize_methods = ['stemming', 'lemmatization']\n",
    "tokenize_steps = [X_train_stems, X_train_lemmas]\n",
    "\n",
    "vectorize_methods = ['count', 'tfidf', 'pretrained_glove', 'custom_word2vec']\n",
    "vectorize_steps = [count_transformer, tfidf_transformer, glove_transformer, word2vec_transformer]\n",
    "\n",
    "classifiers = [MultinomialNB(), LogisticRegression(random_state=seed), RandomForestClassifier(random_state=seed),\n",
    "               KNeighborsClassifier(), SVC(random_state=seed)]\n",
    "clf_names = ['naive_bayes', 'logreg', 'random_forest', 'knn', 'svc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748530e",
   "metadata": {},
   "source": [
    "To save time, try all combinations with lemmatization first. Then, we can try the best ones with stemming to see if we get any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6db1de69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-validate with repeated stratified folds\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "val_strategy = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=seed)\n",
    "\n",
    "val_strategy.get_n_splits(X_train_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "178c7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get cross-validated scores for a combination\n",
    "def get_model_score(val_strategy, X_train, y_train, vectorizer, scaler, classifier):\n",
    "    '''Takes in validation strategy (KFolds), training data, a vectorization strategy, scaling strategy and a\n",
    "    classifier (last 3 as strings); returns the model's average accuracy over all folds in validation strategy.\n",
    "    \n",
    "    val_strategy: instantiated RepeatedStratifiedKFold or KFold object\n",
    "    \n",
    "    X_train: tokenized training data\n",
    "    \n",
    "    y_train: label encoded target for training data\n",
    "    \n",
    "    vectorizer: 'count', 'tfidf', 'glove', 'word2vec'\n",
    "    \n",
    "    scaler: 'mms', 'ss', 'none'\n",
    "    \n",
    "    classifier: 'nb', 'lr', 'rf', 'knn', 'svc' '''\n",
    "    \n",
    "    # options\n",
    "    preprocess = {'count': CountVectorizer(), 'tfidf': TfidfVectorizer(), 'glove': glove_embed,\n",
    "                  'word2vec': custom_embed}\n",
    "    scale = {'mms': MinMaxScaler(), 'ss': StandardScaler()}\n",
    "    classify = {'nb': MultinomialNB(), 'lr': LogisticRegression(), 'rf': RandomForestClassifier(),\n",
    "                'knn': KNeighborsClassifier(), 'svc': SVC()}\n",
    "    \n",
    "    # select options\n",
    "    get_vectors = preprocess[vectorizer]\n",
    "    if scaler != 'none':\n",
    "        scale_data = scale[scaler]\n",
    "    clf = classify[classifier]\n",
    "    \n",
    "    # get model's accuracy score on each fold\n",
    "    score = []\n",
    "\n",
    "    for train_index, val_index in val_strategy.split(X_train, y_train):\n",
    "        # get validation sets for this fold/repeat\n",
    "        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # vectorize\n",
    "        X_train_vectorized = pd.DataFrame(get_vectors.fit_transform(X_train.iloc[train_index]).todense(),\n",
    "                                          index=train_index,\n",
    "                                          columns=get_vectors.vocabulary_)\n",
    "        X_val_vectorized = pd.DataFrame(get_vectors.transform(X_val).todense(), index=val_index,\n",
    "                                        columns=get_vectors.vocabulary_)\n",
    "\n",
    "        # scale (optional)\n",
    "        if scaler != 'none':\n",
    "            X_train_scaled = pd.DataFrame(scale_data.fit_transform(X_train_vectorized), index=train_index,\n",
    "                                          columns=get_vectors.vocabulary_)\n",
    "            X_val_scaled = pd.DataFrame(scale_data.transform(X_val_vectorized), index=val_index,\n",
    "                                        columns=get_vectors.vocabulary_)\n",
    "        else:\n",
    "            X_train_scaled = X_train_vectorized.copy()\n",
    "            X_val_scaled = X_val_vectorized.copy()\n",
    "            \n",
    "        # fit classifier and score\n",
    "        clf.fit(X_train_scaled, y_train.iloc[train_index])\n",
    "        score.append(clf.score(X_val_scaled, y_val))\n",
    "\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "f6d1b47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9432710287288953"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get cross-validated score for lemmatize/count-vectorize/random forest\n",
    "\n",
    "get_model_score(val_strategy, X_train_lemmas, y_train, 'count', 'none', 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b8355600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9394237496475113\n"
     ]
    }
   ],
   "source": [
    "# get cross-validated score for lemmatize/count-vectorize/naive bayes\n",
    "score = []\n",
    "\n",
    "for train_index, val_index in val_strategy.split(X_train_lemmas, y_train):\n",
    "    # get validation sets for this fold/repeat\n",
    "    X_val, y_val = X_train_lemmas.iloc[val_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # vectorize\n",
    "    count = CountVectorizer()\n",
    "    X_train_vectorized = pd.DataFrame(count.fit_transform(X_train.iloc[train_index]).todense(),\n",
    "                                      index=train_index,\n",
    "                                      columns=count.vocabulary_)\n",
    "    X_val_vectorized = pd.DataFrame(count.transform(X_val).todense(), index=val_index, columns=count.vocabulary_)\n",
    "    \n",
    "    # scale\n",
    "    mms = MinMaxScaler()\n",
    "    X_train_scaled = pd.DataFrame(mms.fit_transform(X_train_vectorized), index=train_index, columns=count.vocabulary_)\n",
    "    X_val_scaled = pd.DataFrame(mms.transform(X_val_vectorized), index=val_index, columns=count.vocabulary_)\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_scaled, y_train.iloc[train_index])\n",
    "    score.append(clf.score(X_val_scaled, y_val))\n",
    "    \n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b609d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9409625731946093\n"
     ]
    }
   ],
   "source": [
    "# get cross-validated score for lemmatize/count-vectorize/SVC\n",
    "score = []\n",
    "\n",
    "for train_index, val_index in val_strategy.split(X_train_lemmas, y_train):\n",
    "    # get validation sets for this fold/repeat\n",
    "    X_val, y_val = X_train_lemmas.iloc[val_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # vectorize\n",
    "    count = CountVectorizer()\n",
    "    X_train_vectorized = pd.DataFrame(count.fit_transform(X_train.iloc[train_index]).todense(),\n",
    "                                      index=train_index,\n",
    "                                      columns=count.vocabulary_)\n",
    "    X_val_vectorized = pd.DataFrame(count.transform(X_val).todense(), index=val_index, columns=count.vocabulary_)\n",
    "    \n",
    "    # scale\n",
    "    mms = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(mms.fit_transform(X_train_vectorized), index=train_index, columns=count.vocabulary_)\n",
    "    X_val_scaled = pd.DataFrame(mms.transform(X_val_vectorized), index=val_index, columns=count.vocabulary_)\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = SVC()\n",
    "    clf.fit(X_train_scaled, y_train.iloc[train_index])\n",
    "    score.append(clf.score(X_val_scaled, y_val))\n",
    "    \n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "55f17503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('preprocess', count_transformer),\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('classify', MultinomialNB())\n",
    "])\n",
    "\n",
    "cross_val_score(pipe, X_train_lemmas, y_train, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4d925cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 105, in predict\n",
      "    X = self._check_X(X)\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 579, in _check_X\n",
      "    return self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 105, in predict\n",
      "    X = self._check_X(X)\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 579, in _check_X\n",
      "    return self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 105, in predict\n",
      "    X = self._check_X(X)\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 579, in _check_X\n",
      "    return self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 105, in predict\n",
      "    X = self._check_X(X)\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 579, in _check_X\n",
      "    return self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 105, in predict\n",
      "    X = self._check_X(X)\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/naive_bayes.py\", line 579, in _check_X\n",
      "    return self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 419, in predict\n",
      "    scores = self.decision_function(X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 400, in decision_function\n",
      "    X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 419, in predict\n",
      "    scores = self.decision_function(X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 400, in decision_function\n",
      "    X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 419, in predict\n",
      "    scores = self.decision_function(X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 400, in decision_function\n",
      "    X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 419, in predict\n",
      "    scores = self.decision_function(X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 400, in decision_function\n",
      "    X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 419, in predict\n",
      "    scores = self.decision_function(X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py\", line 400, in decision_function\n",
      "    X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[208], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m steps\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m'\u001b[39m, classify))\n\u001b[1;32m     18\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline(steps)\n\u001b[0;32m---> 19\u001b[0m score \u001b[38;5;241m=\u001b[39m cross_val_score(pipe, training_data, y_train, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenization_method\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorization_method\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(vectorize_methods[i])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    474\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    475\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    476\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    477\u001b[0m )(\n\u001b[1;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    479\u001b[0m         t,\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[1;32m    481\u001b[0m         X,\n\u001b[1;32m    482\u001b[0m         y,\n\u001b[1;32m    483\u001b[0m         sample_weight,\n\u001b[1;32m    484\u001b[0m         i,\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[1;32m    486\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    487\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[1;32m    488\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 184\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data = tokenize_steps[1]\n",
    "\n",
    "for i in range(len(vectorize_steps)):\n",
    "    for j in range(len(classifiers)):\n",
    "        preprocess = vectorize_steps[i]\n",
    "        classify = classifiers[j]\n",
    "        \n",
    "        steps = []\n",
    "        steps.append(('preprocess', preprocess))\n",
    "            \n",
    "        if clf_name[j] == 'naive_bayes':\n",
    "            steps.append(('scale', MinMaxScaler()))\n",
    "        elif clf_name[j] in ['logreg', 'knn', 'svc']:\n",
    "            steps.append(('scale', StandardScaler()))\n",
    "            \n",
    "        steps.append(('classify', classify))\n",
    "        \n",
    "        pipe = Pipeline(steps)\n",
    "        score = cross_val_score(pipe, training_data, y_train, scoring='accuracy')\n",
    "        \n",
    "        results['tokenization_method'].append('lemmatize')\n",
    "        results['vectorization_method'].append(vectorize_methods[i])\n",
    "        results['classifier'].append(clf_names[j])\n",
    "        results['score'].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "17659c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 277, in compute\n",
      "    return ArgKmin64.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 95, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 234, in predict\n",
      "    neigh_ind = self.kneighbors(X, return_distance=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 824, in kneighbors\n",
      "    results = ArgKmin.compute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\", line 289, in compute\n",
      "    return ArgKmin32.compute(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\", line 584, in sklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/fixes.py\", line 139, in threadpool_limits\n",
      "    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 171, in __init__\n",
      "    self._original_info = self._set_threadpool_limits()\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 268, in _set_threadpool_limits\n",
      "    modules = _ThreadpoolInfo(prefixes=self._prefixes,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 340, in __init__\n",
      "    self._load_modules()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 371, in _load_modules\n",
      "    self._find_modules_with_dyld()\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 428, in _find_modules_with_dyld\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 508, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310apple\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310apple\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 480, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 992, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310apple\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 10am\n",
      "- 10x\n",
      "- 1223\n",
      "- 125\n",
      "- 1990style\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 100tc\n",
      "- 12bn\n",
      "- 1408\n",
      "- 141st\n",
      "- 1443\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 06\n",
      "- 12ab\n",
      "- 1m\n",
      "- 206k\n",
      "- 250\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 0310appl\n",
      "- 08\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 0310appl\n",
      "- 08\n",
      "- 10x2\n",
      "- 1154\n",
      "- 11ntc\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 00am\n",
      "- 00pm\n",
      "- 01am\n",
      "- 06\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 820, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 862, in predict_proba\n",
      "    X = self._validate_X_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 602, in _validate_X_predict\n",
      "    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 548, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/base.py\", line 481, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- 00pm\n",
      "- 01am\n",
      "- 10k\n",
      "- 11bil\n",
      "- 14th\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 02\n",
      "- 0310appl\n",
      "- 06\n",
      "- 08\n",
      "- 100tc\n",
      "- ...\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_cell_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# store scores for each combination of options\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mresults = \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenization_method\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [], \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorization_method\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [], \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [], \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: []}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# store names and data / transformers / estimators needed\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtokenize_methods = [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemming\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatization\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtokenize_steps = [X_train_stems, X_train_lemmas]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mvectorize_methods = [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_glove\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_word2vec\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mvectorize_steps = [count_transformer, tfidf_transformer, glove_transformer, word2vec_transformer]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mclassifiers = [MultinomialNB(), LogisticRegression(random_state=seed), RandomForestClassifier(random_state=seed),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m               KNeighborsClassifier(), SVC(random_state=seed)]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mclf_names = [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaive_bayes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogreg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# iterate over every combination of classifier, tokenization method and vectorization method\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfor clf_name, classifier in zip(clf_names, classifiers):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    for tokenization_method, tokenized_tweets in zip(tokenize_methods, tokenize_steps):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        for vectorization_method, vectorizer in zip(vectorize_methods, vectorize_steps):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            # store steps for pipeline\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            steps = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            steps.append((\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, vectorizer))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            if clf_name == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaive_bayes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m                steps.append((\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, MinMaxScaler()))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            elif clf_name in [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogreg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m                steps.append((\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, StandardScaler()))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            steps.append((\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, classifier))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            # store results\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            pipe = Pipeline(steps)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            score = cross_val_score(pipe, tokenized_tweets, y_train, scoring=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            results[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenization_method\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m].append(tokenization_method)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            results[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorization_method\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m].append(vectorization_method)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            results[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m].append(clf_name)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            results[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m].append(score)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2475\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2474\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2475\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2477\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2478\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/IPython/core/magics/execution.py:1174\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m all_runs \u001b[38;5;241m=\u001b[39m timer\u001b[38;5;241m.\u001b[39mrepeat(repeat, number)\n\u001b[1;32m   1175\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n\u001b[1;32m   1176\u001b[0m worst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/timeit.py:206\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    204\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[0;32m--> 206\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeit(number)\n\u001b[1;32m    207\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/IPython/core/magics/execution.py:158\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    156\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner(it, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:38\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    474\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    475\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    476\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    477\u001b[0m )(\n\u001b[1;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    479\u001b[0m         t,\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[1;32m    481\u001b[0m         X,\n\u001b[1;32m    482\u001b[0m         y,\n\u001b[1;32m    483\u001b[0m         sample_weight,\n\u001b[1;32m    484\u001b[0m         i,\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[1;32m    486\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    487\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[1;32m    488\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 184\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# iterate over every combination of classifier, tokenization method and vectorization method\n",
    "\n",
    "for clf_name, classifier in zip(clf_names, classifiers):\n",
    "    for tokenization_method, tokenized_tweets in zip(tokenize_methods, tokenize_steps):\n",
    "        for vectorization_method, vectorizer in zip(vectorize_methods, vectorize_steps):\n",
    "            \n",
    "            # store steps for pipeline\n",
    "            \n",
    "            steps = []\n",
    "            steps.append(('vectorize', vectorizer))\n",
    "            \n",
    "            if clf_name == 'naive_bayes':\n",
    "                steps.append(('scale', MinMaxScaler()))\n",
    "            elif clf_name in ['logreg', 'knn', 'svc']:\n",
    "                steps.append(('scale', StandardScaler()))\n",
    "            \n",
    "            steps.append(('classify', classifier))\n",
    "            \n",
    "            # store results\n",
    "            \n",
    "            pipe = Pipeline(steps)\n",
    "            score = cross_val_score(pipe, tokenized_tweets, y_train, scoring='accuracy')\n",
    "            \n",
    "            results['tokenization_method'].append(tokenization_method)\n",
    "            results['vectorization_method'].append(vectorization_method)\n",
    "            results['classifier'].append(clf_name)\n",
    "            results['score'].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "aeea4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7e3e7e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93916084, 0.93916084, 0.93916084, 0.93981805, 0.93911826])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pipe = Pipeline([\n",
    "    ('vectorize', glove_transformer),\n",
    "    ('scale', mms),\n",
    "    ('classify', MultinomialNB())\n",
    "])\n",
    "\n",
    "cross_val_score(test_pipe, X_train_stems, y_train, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e0c8e859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93916084, 0.93916084, 0.93916084, 0.93981805, 0.93911826])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pipe = Pipeline([\n",
    "    ('vectorize', word2vec_transformer),\n",
    "    ('scale', mms),\n",
    "    ('classify', MultinomialNB())\n",
    "])\n",
    "\n",
    "cross_val_score(test_pipe, X_train_stems, y_train, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e266f5eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m ct \u001b[38;5;241m=\u001b[39m make_column_transformer(\n\u001b[1;32m      4\u001b[0m     (lemma_transformer, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_text\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      5\u001b[0m     (count, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m test_pipe \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m      9\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform_tweets\u001b[39m\u001b[38;5;124m'\u001b[39m, ct),\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m'\u001b[39m, MultinomialNB())\n\u001b[1;32m     11\u001b[0m ])\n\u001b[0;32m---> 13\u001b[0m test_pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    360\u001b[0m     cloned_transformer,\n\u001b[1;32m    361\u001b[0m     X,\n\u001b[1;32m    362\u001b[0m     y,\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    364\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    365\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:724\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m--> 724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:426\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    425\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 426\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m _get_column_indices(X, columns)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/__init__.py:404\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_column_indices\u001b[39m(X, key):\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;124;03m\"\"\"Get feature column indices for input data X and key.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    For accepted values of `key`, see the docstring of\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    :func:`_safe_indexing_column`.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m     n_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    406\u001b[0m     key_dtype \u001b[38;5;241m=\u001b[39m _determine_key_type(key)\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;66;03m# we get an empty list\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (lemma_transformer, ['tweet_text']),\n",
    "    (count, ['tweet_text'])\n",
    ")\n",
    "\n",
    "test_pipe = Pipeline([\n",
    "    ('transform_tweets', ct),\n",
    "    ('classify', MultinomialNB())\n",
    "])\n",
    "\n",
    "test_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "63b97837",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "keywords must be strings",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_step \u001b[38;5;129;01min\u001b[39;00m tokenize_steps:\n\u001b[1;32m      6\u001b[0m     baseline \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m      7\u001b[0m         [(token_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      8\u001b[0m          (vector_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorize\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      9\u001b[0m          (clf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     10\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0m     score \u001b[38;5;241m=\u001b[39m cross_val_score(baseline, X_train, y_train, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     baseline_results\u001b[38;5;241m.\u001b[39mappend((token_step, vector_step, clf, score))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: keywords must be strings"
     ]
    }
   ],
   "source": [
    "baseline_results = []\n",
    "\n",
    "for clf in classifiers:\n",
    "    for vector_step in vectorize_steps:\n",
    "        for token_step in tokenize_steps:\n",
    "            ct = make_column_transformer()\n",
    "            baseline = Pipeline(\n",
    "                [(token_step, 'tokenize'),\n",
    "                 (vector_step, 'vectorize'),\n",
    "                 (clf, 'classify')]\n",
    "            )\n",
    "            score = cross_val_score(baseline, X_train, y_train, scoring='accuracy', error_score='raise')\n",
    "            baseline_results.append((token_step, vector_step, clf, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8874e7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c3f646a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'fit_predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m baseline\u001b[38;5;241m.\u001b[39mfit_predict(X_train, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/utils/_available_if.py:32\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m     26\u001b[0m attr_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(owner\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck(obj):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[1;32m     34\u001b[0m     out \u001b[38;5;241m=\u001b[39m MethodType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py:46\u001b[0m, in \u001b[0;36m_final_estimator_has.<locals>.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# raise original `AttributeError` if `attr` does not exist\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator, attr)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'fit_predict'"
     ]
    }
   ],
   "source": [
    "baseline.fit_predict(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "224d2a80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: keywords must be strings\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[0;32m----> 3\u001b[0m cross_val_score(baseline, X_train, y_train, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m    266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[0;32m--> 285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/user/opt/anaconda3/envs/twitter-env/lib/python3.11/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: keywords must be strings\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(baseline, X_train, y_train, scoring='accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-env",
   "language": "python",
   "name": "twitter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
